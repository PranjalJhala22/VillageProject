{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc47e61-d36f-4266-b018-74f3931e8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Data Processing Pipeline for Village Cinemas F&B Analytics.\n",
    "\n",
    "This script consolidates, cleans, and transforms raw transaction and session\n",
    "data into analysis-ready feature matrices. It serves as the primary data\n",
    "engineering step for both the training and test datasets.\n",
    "\n",
    "The pipeline executes the following main stages:\n",
    "1.  **Inventory Transactions Cleanup:**\n",
    "    -   Loads and combines all raw `Inventory Transaction Data*.xlsx` files.\n",
    "    -   Normalizes headers, filters for relevant F&B classes, and parses timestamps.\n",
    "    -   Imputes missing prices using a hierarchical median approach.\n",
    "    -   Outputs a consolidated `inventory_transactions_clean.xlsx`.\n",
    "\n",
    "2.  **One-Hot Encoding (OHE) of Transactions:**\n",
    "    -   Takes the cleaned transaction data.\n",
    "    -   Creates wide-format matrices with one-hot encoded item classes and product names,\n",
    "        aggregated by the transaction timestamp.\n",
    "    -   Outputs intermediate files like `ohe_trx_item_class_product.xlsx`.\n",
    "\n",
    "3.  **Movie Sessions Cleanup:**\n",
    "    -   Loads and cleans raw `Movie_sessions*.xlsx` files.\n",
    "    -   Applies business rules, such as filtering by genre and session time.\n",
    "    -   Engineers features like duration categories and time-of-day slots.\n",
    "    -   Outputs a consolidated `movie_sessions_clean.xlsx`.\n",
    "\n",
    "4.  **Hourly Session Expansion:**\n",
    "    -   Expands session data to represent every hour a session is active, including\n",
    "      pre-session buffer times for early arrivals.\n",
    "    -   This creates a richer, hour-level view of cinema activity.\n",
    "    -   Outputs `ohe_movie_sessions_hourly_expanded.xlsx`.\n",
    "\n",
    "5.  **Final Merge:**\n",
    "    -   Combines the OHE transaction data with the expanded hourly session data using a\n",
    "      left join on the timestamp.\n",
    "    -   This produces the final `train_dataset.xlsx` or `test_dataset.xlsx`, which is\n",
    "      the primary input for the downstream recommendation models.\n",
    "\n",
    "To run, place raw data files in the `input/` subdirectory and execute this script.\n",
    "All outputs will be saved to the `output/` subdirectory.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Final, Dict, Tuple, List\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# --- 1. CONFIGURATION AND CONSTANTS ---\n",
    "# =============================================================================\n",
    "\n",
    "# --- Path Configuration ---\n",
    "BASE_DIR: Final[Path] = Path(\".\")\n",
    "# Assuming raw files are in an 'input' folder and outputs go to an 'output' folder\n",
    "INPUT_DIR: Final[Path] = BASE_DIR / \"input\"\n",
    "OUTPUT_DIR: Final[Path] = BASE_DIR / \"output\"\n",
    "\n",
    "# --- Regex Patterns for File Discovery ---\n",
    "WORKBOOK_RX_INV: Final[re.Pattern[str]] = re.compile(\n",
    "    r\"Inventory Transaction Data \"\n",
    "    r\"(?:\"\n",
    "    r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}|\\d{4}\"\n",
    "    r\")\"\n",
    "    r\"(?:\\s+v\\d+(?:\\.\\d+)?)?\"\n",
    "    r\"\\.xlsx$\",\n",
    "    re.I\n",
    ")\n",
    "SOURCE_RX_SESS: Final[re.Pattern[str]] = re.compile(\n",
    "    r\"Movie_sessions\"\n",
    "    r\"(?:_(?:(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s?\\d{4}))?\"\n",
    "    r\"(?:\\s+v\\d+(?:\\.\\d+)?)?\"\n",
    "    r\"\\.xlsx$\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "# --- Business Logic Constants ---\n",
    "ALLOWED_CLASSES: Final[set[str]] = {\n",
    "    \"SNACK - CHIPS\", \"FOOD - VJUNIOR\", \"ICE CREAMS - OTHER\", \"ICE CREAMS - CHOC TO\",\n",
    "    \"DRINKS - EXTRA LARGE\", \"DRINKS - LARGE\", \"DRINKS - MEDIUM\", \"DRINKS - SMALL\",\n",
    "    \"DRINKS - NO ICE\", \"DRINKS\", \"POPCORN\",\n",
    "}\n",
    "EXCLUDE_GENRES: Final[set[str]] = {\"GAMING\", \"TO BE ADVISED\"}\n",
    "\n",
    "SLOT_WINDOWS: Final[Dict[str, Tuple[int, int]]] = {\n",
    "    \"morning\": (9*60, 11*60), \"early_noon\": (11*60, 13*60), \"noon\": (13*60, 15*60),\n",
    "    \"late_noon\": (15*60, 17*60), \"evening_1\": (17*60, 17*60 + 30), \"evening_2\": (17*60 + 30, 18*60),\n",
    "    \"evening_3\": (18*60, 18*60 + 15), \"evening_4\": (18*60 + 15, 18*60 + 30),\n",
    "    \"evening_5\": (18*60 + 30, 18*60 + 45), \"evening_6\": (18*60 + 45, 19*60),\n",
    "    \"night_1\": (19*60, 19*60 + 15), \"night_2\": (19*60 + 15, 19*60 + 30),\n",
    "    \"night_3\": (19*60 + 30, 20*60), \"night_4\": (20*60, 20*60 + 30),\n",
    "    \"night_5\": (20*60 + 30, 21*60), \"night_6\": (21*60, 22*60),\n",
    "}\n",
    "SHORT_MAX_SESS: Final[int] = 120\n",
    "MEDIUM_MAX_SESS: Final[int] = 160\n",
    "\n",
    "PRE_BUFFER_HRS: Final[int] = 1\n",
    "POST_BUFFER_HRS: Final[int] = 0\n",
    "\n",
    "# CORRECTED: Added this constant from the notebook, which caused the NameError\n",
    "CAT_COLS_SESS: Final[List[str]] = [\n",
    "    \"language\", \"genre\", \"rating\", \"slot\", \"duration_category\"\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# --- 2. HELPER SUB-FUNCTIONS ---\n",
    "# =============================================================================\n",
    "\n",
    "def _find_header_row(xl_path: Path, sheet_name: str, start_col_name: str) -> int:\n",
    "    \"\"\"Locate the header row that contains a specific starting column name.\"\"\"\n",
    "    raw = pd.read_excel(xl_path, sheet_name=sheet_name, header=None, dtype=str)\n",
    "    for idx, row in raw.iterrows():\n",
    "        if any(str(cell).lower().startswith(start_col_name) for cell in row):\n",
    "            return idx\n",
    "    raise ValueError(f\"{xl_path.name}: header row with '{start_col_name}' not found\")\n",
    "\n",
    "def _remove_no_flag_inv(text: str) -> str:\n",
    "    return re.compile(r\"\\bNO\\s+(ICE|SUGAR)\\b\", re.I).sub(\"\", str(text)).replace(\"  \", \" \").strip()\n",
    "\n",
    "def _slot_from_timestamp_sess(ts: pd.Timestamp) -> str:\n",
    "    minutes = ts.hour * 60 + ts.minute\n",
    "    for slot, (start, end) in SLOT_WINDOWS.items():\n",
    "        if start <= minutes < end: return slot\n",
    "    return \"out_of_range\"\n",
    "\n",
    "def _duration_bucket_sess(minutes: int) -> str:\n",
    "    if minutes <= SHORT_MAX_SESS: return \"short\"\n",
    "    if minutes <= MEDIUM_MAX_SESS: return \"medium\"\n",
    "    return \"long\"\n",
    "\n",
    "def _parse_duration_sess(duration_text: str) -> int:\n",
    "    match = re.search(r\"(\\d+)\", str(duration_text))\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def _build_ohe_matrix(df: pd.DataFrame, cat_cols: list[str], *, keep_prefix: bool = False) -> pd.DataFrame:\n",
    "    if \"quantity\" not in df.columns: raise KeyError(\"Missing 'quantity' column.\")\n",
    "    dummy_kwargs = {} if keep_prefix else dict(prefix=\"\", prefix_sep=\"\")\n",
    "    dummies = pd.get_dummies(df[cat_cols], columns=cat_cols, dtype=\"uint32\", **dummy_kwargs)\n",
    "    weighted = dummies.mul(df[\"quantity\"].values, axis=0)\n",
    "    return weighted.groupby(df[\"timestamp\"]).sum().astype(\"uint32\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- 3. CORE DATA PROCESSING FUNCTIONS ---\n",
    "# =============================================================================\n",
    "\n",
    "def _load_and_clean_file_inv(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Return a cleaned DataFrame for one source inventory workbook.\"\"\"\n",
    "    print(f\"→ Cleaning inventory file: {xl_path.name}\")\n",
    "    header_row = _find_header_row(xl_path, \"Inventory Trans\", \"transaction date\")\n",
    "    df_raw = pd.read_excel(xl_path, sheet_name=\"Inventory Trans\", header=header_row, dtype=str)\n",
    "    df = df_raw[~df_raw[\"Transaction Date\"].str.contains(\"result\", case=False, na=False)].copy()\n",
    "\n",
    "    rename_map = {}\n",
    "    for col in df.columns:\n",
    "        key = str(col).lower()\n",
    "        if \"no of items\" in key or key.strip() == \"ea\": rename_map[col] = \"quantity\"\n",
    "        elif \"sell price\" in key or key.strip() == \"aud\": rename_map[col] = \"price_aud\"\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    df.drop(columns=[c for c in df.columns if str(c).startswith('Unnamed')], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    df[\"Transaction Date\"] = pd.to_datetime(df[\"Transaction Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    df[\"Transaction Hour\"] = pd.to_numeric(df[\"Transaction Hour\"], errors=\"coerce\")\n",
    "    df.dropna(subset=[\"Transaction Date\", \"Transaction Hour\"], inplace=True)\n",
    "    df[\"Transaction Hour\"] = df[\"Transaction Hour\"].astype(int)\n",
    "\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        if col in df.columns: df[col] = df[col].str.strip()\n",
    "    df[\"Item Class\"] = df[\"Item Class\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df = df[df[\"Item Class\"].str.upper().isin(ALLOWED_CLASSES)].copy()\n",
    "\n",
    "    df[\"quantity\"] = pd.to_numeric(df[\"quantity\"], errors=\"coerce\").fillna(1).clip(lower=1).astype(int)\n",
    "    df[\"price_aud\"] = pd.to_numeric(df[\"price_aud\"], errors=\"coerce\")\n",
    "\n",
    "    df[\"timestamp\"] = df[\"Transaction Date\"].dt.normalize() + pd.to_timedelta(df[\"Transaction Hour\"], unit=\"h\")\n",
    "\n",
    "    # --- Price imputation hierarchy ---\n",
    "    df[\"unit_price\"] = df[\"price_aud\"] / df[\"quantity\"]\n",
    "    \n",
    "    # 1) Exact product match map\n",
    "    unit_price_map = df[df[\"unit_price\"] > 0].drop_duplicates(\"VISTA Item\").set_index(\"VISTA Item\")[\"unit_price\"]\n",
    "    \n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] <= 0)\n",
    "    if missing_mask.any(): df.loc[missing_mask, \"unit_price\"] = df.loc[missing_mask, \"VISTA Item\"].map(unit_price_map)\n",
    "\n",
    "    # 2) \"NO ICE / NO SUGAR\" proxy\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] <= 0)\n",
    "    if missing_mask.any(): df.loc[missing_mask, \"unit_price\"] = df.loc[missing_mask, \"VISTA Item\"].apply(_remove_no_flag_inv).map({k: v for k, v in unit_price_map.items()})\n",
    "\n",
    "    # 3) CORRECTED: Added missing imputation step for SNACK - CHIPS\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] <= 0)\n",
    "    chips_mask = missing_mask & (df[\"Item Class\"] == \"SNACK - CHIPS\")\n",
    "    if chips_mask.any():\n",
    "        chips_median = df.loc[df[\"Item Class\"] == \"SNACK - CHIPS\", \"unit_price\"].median()\n",
    "        df.loc[chips_mask, \"unit_price\"] = chips_median\n",
    "\n",
    "    # 4) Item-class median\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] <= 0)\n",
    "    if missing_mask.any():\n",
    "        class_medians = df[df[\"unit_price\"] > 0].groupby(\"Item Class\")[\"unit_price\"].median()\n",
    "        df.loc[missing_mask, \"unit_price\"] = df.loc[missing_mask, \"Item Class\"].map(class_medians)\n",
    "\n",
    "    # 5) Global median\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] <= 0)\n",
    "    if missing_mask.any(): df.loc[missing_mask, \"unit_price\"] = df[\"unit_price\"].median(skipna=True)\n",
    "\n",
    "    df[\"price_aud\"] = (df[\"unit_price\"] * df[\"quantity\"]).round(2)\n",
    "\n",
    "    return df.rename(columns={\"Item Class\": \"item_class\", \"VISTA Item\": \"product_name\"})[\n",
    "        [\"timestamp\", \"item_class\", \"product_name\", \"quantity\", \"price_aud\"]\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "def _clean_one_workbook_sess(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load, clean, and return a standardized DataFrame for one session workbook.\"\"\"\n",
    "    print(f\"→ Cleaning session file: {xl_path.name}\")\n",
    "    header_row = _find_header_row(xl_path, \"Sheet1\", \"session date\")\n",
    "    df = pd.read_excel(xl_path, sheet_name=\"Sheet1\", header=header_row, dtype=str)\n",
    "    df = df.drop(columns={\"Film\"}, errors=\"ignore\")\n",
    "    df[\"Session Date\"] = pd.to_datetime(df[\"Session Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    df[\"Session Hour\"] = pd.to_numeric(df[\"Session Hour\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Session Date\", \"Session Hour\", \"Genre\", \"Total Admits\", \"Duration\"])\n",
    "    df[\"Session Hour\"] = df[\"Session Hour\"].astype(int)\n",
    "    df = df[~df[\"Genre\"].str.upper().str.strip().isin(EXCLUDE_GENRES)]\n",
    "    df[\"timestamp\"] = df[\"Session Date\"].dt.normalize() + pd.to_timedelta(df[\"Session Hour\"], unit=\"h\")\n",
    "    df[\"duration_min\"] = df[\"Duration\"].apply(_parse_duration_sess)\n",
    "    df = df[df[\"duration_min\"] != 960]\n",
    "    df[\"duration_category\"] = df[\"duration_min\"].apply(_duration_bucket_sess)\n",
    "    df[\"slot\"] = df[\"timestamp\"].apply(_slot_from_timestamp_sess)\n",
    "    df = df[df[\"slot\"] != \"out_of_range\"]\n",
    "    df = df.rename(columns={\"Session Audio Language\": \"language\", \"Genre\": \"genre\", \"Censor Rating\": \"rating\", \"Total Admits\": \"admits\"})\n",
    "    df[\"admits\"] = pd.to_numeric(df[\"admits\"], errors='coerce').fillna(0).astype(int)\n",
    "    return df[[\"timestamp\", \"language\", \"genre\", \"rating\", \"admits\", \"duration_min\", \"duration_category\", \"slot\"]].sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- 4. PIPELINE STAGE FUNCTIONS ---\n",
    "# =============================================================================\n",
    "\n",
    "def clean_inventory_transactions(input_dir: Path, output_dir: Path) -> Path | None:\n",
    "    \"\"\"Consolidates and cleans raw inventory transaction Excel files.\"\"\"\n",
    "    print(\"--- Stage 1: Cleaning Inventory Transactions ---\")\n",
    "    source_files = sorted(p for p in input_dir.iterdir() if WORKBOOK_RX_INV.fullmatch(p.name))\n",
    "    if not source_files:\n",
    "        print(f\"📭 No source inventory workbooks found in '{input_dir}/'. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    cleaned_frames = [_load_and_clean_file_inv(p) for p in source_files]\n",
    "    master = pd.concat(cleaned_frames, ignore_index=True).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    print(f\"\\n✅ Cleaned inventory rows total: {len(master):,}\")\n",
    "\n",
    "    master_output_path = output_dir / \"inventory_transactions_clean.xlsx\"\n",
    "    master.to_excel(master_output_path, index=False)\n",
    "    print(f\"  • {master_output_path.name} written\")\n",
    "\n",
    "    for year, group in master.groupby(master[\"timestamp\"].dt.year, sort=True):\n",
    "        fname = f\"inventory_transactions_clean_{year}.xlsx\"\n",
    "        year_output_path = output_dir / fname\n",
    "        group.to_excel(year_output_path, index=False)\n",
    "        print(f\"  • {year_output_path.name}  ({len(group):,} rows)\")\n",
    "\n",
    "    return master_output_path\n",
    "\n",
    "\n",
    "def one_hot_encode_transactions(input_path: Path, output_dir: Path) -> None:\n",
    "    \"\"\"Builds one-hot encoded (OHE) matrices from cleaned transaction data.\"\"\"\n",
    "    print(\"\\n--- Stage 2: One-Hot Encoding Transactions ---\")\n",
    "    if not input_path or not input_path.exists():\n",
    "        print(f\"⚠️ Source file for OHE not found at '{input_path}'. Skipping OHE.\")\n",
    "        return\n",
    "\n",
    "    transactions = pd.read_excel(input_path, parse_dates=[\"timestamp\"])\n",
    "    transactions[\"item_class\"] = transactions[\"item_class\"].str.upper().str.strip()\n",
    "    transactions[\"product_name\"] = transactions[\"product_name\"].str.upper().str.strip()\n",
    "\n",
    "    hourly_revenue = transactions.groupby(\"timestamp\")[\"price_aud\"].sum().round(2).rename(\"total_price_aud\")\n",
    "\n",
    "    ohe_combined = _build_ohe_matrix(transactions, [\"item_class\", \"product_name\"], keep_prefix=True)\n",
    "    ohe_combined.insert(0, \"total_price_aud\", hourly_revenue)\n",
    "\n",
    "    # We only need the combined file for the final merge\n",
    "    path = output_dir / \"ohe_trx_item_class_product.xlsx\"\n",
    "    ohe_combined.to_excel(path)\n",
    "    print(f\"  • {path.name} written\")\n",
    "\n",
    "\n",
    "def clean_movie_sessions(input_dir: Path, output_dir: Path) -> Path | None:\n",
    "    \"\"\"Consolidates and cleans raw movie session Excel files.\"\"\"\n",
    "    print(\"\\n--- Stage 3: Cleaning Movie Sessions ---\")\n",
    "    workbooks = sorted(p for p in input_dir.iterdir() if SOURCE_RX_SESS.fullmatch(p.name))\n",
    "    if not workbooks:\n",
    "        print(f\"📭 No source movie session workbooks found in '{input_dir}/'. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    frames = [_clean_one_workbook_sess(p) for p in workbooks]\n",
    "    combined = pd.concat(frames, ignore_index=True).sort_values(\"timestamp\")\n",
    "    print(f\"\\n✅ Total cleaned sessions: {len(combined):,}\")\n",
    "\n",
    "    combined_output_path = output_dir / \"movie_sessions_clean.xlsx\"\n",
    "    combined.to_excel(combined_output_path, index=False)\n",
    "    print(f\"• {combined_output_path.name} written\")\n",
    "\n",
    "    for year, grp in combined.groupby(combined[\"timestamp\"].dt.year, sort=True):\n",
    "        fname = f\"movie_sessions_clean_{year}.xlsx\"\n",
    "        year_output_path = output_dir / fname\n",
    "        grp.to_excel(year_output_path, index=False)\n",
    "        print(f\"• {year_output_path.name} ({len(grp):,} rows)\")\n",
    "\n",
    "    return combined_output_path\n",
    "\n",
    "\n",
    "def expand_and_ohe_sessions(input_path: Path, output_dir: Path) -> Path | None:\n",
    "    \"\"\"Expands session data to an hourly level and one-hot encodes categorical features.\"\"\"\n",
    "    print(\"\\n--- Stage 4: Expanding & OHE Sessions ---\")\n",
    "    if not input_path or not input_path.exists():\n",
    "        print(f\"⚠️ Source file for session expansion not found at '{input_path}'. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    sessions = pd.read_excel(input_path, parse_dates=[\"timestamp\"])\n",
    "    sessions[\"end_time\"] = sessions[\"timestamp\"] + pd.to_timedelta(sessions[\"duration_min\"], unit=\"m\")\n",
    "\n",
    "    expanded_rows = []\n",
    "    for _, sess in sessions.iterrows():\n",
    "        site_start = sess[\"timestamp\"] - timedelta(hours=PRE_BUFFER_HRS)\n",
    "        site_end = sess[\"end_time\"] + timedelta(hours=POST_BUFFER_HRS)\n",
    "        for hr in pd.date_range(site_start, site_end, freq=\"h\", inclusive=\"left\"):\n",
    "            row = sess.copy()\n",
    "            row[\"timestamp\"] = hr\n",
    "            # CORRECTED: Replicated notebook logic for in_site and in_show flags\n",
    "            row[\"in_site\"] = int(site_start <= hr < site_end)\n",
    "            row[\"in_show\"] = int(sess[\"timestamp\"] <= hr < sess[\"end_time\"])\n",
    "            expanded_rows.append(row)\n",
    "\n",
    "    expanded = pd.DataFrame(expanded_rows).drop(columns=[\"end_time\"])\n",
    "\n",
    "    # CORRECTED: Used the defined CAT_COLS_SESS to fix the NameError\n",
    "    ohe = pd.get_dummies(expanded, columns=CAT_COLS_SESS, prefix=CAT_COLS_SESS, prefix_sep=\"_\", dtype=\"uint8\")\n",
    "    cat_dummies = [c for c in ohe.columns if any(c.startswith(f\"{cat}_\") for cat in CAT_COLS_SESS)]\n",
    "    ohe[cat_dummies] = ohe[cat_dummies].mul(ohe[\"in_show\"], axis=0)\n",
    "\n",
    "    # CORRECTED: Replicated the exact admits calculation from the notebook\n",
    "    ohe[\"admits_in_site\"] = ohe[\"admits\"] * ohe[\"in_site\"]\n",
    "    ohe[\"admits_in_show\"] = ohe[\"admits\"] * ohe[\"in_show\"]\n",
    "    ohe[\"dur_x_admits\"] = ohe[\"duration_min\"] * ohe[\"admits\"] * ohe[\"in_show\"]\n",
    "\n",
    "    agg_map = {c: \"sum\" for c in cat_dummies}\n",
    "    agg_map.update({\n",
    "        \"admits_in_site\": \"sum\",\n",
    "        \"admits_in_show\": \"sum\",\n",
    "        \"dur_x_admits\": \"sum\",\n",
    "    })\n",
    "\n",
    "    hourly = ohe.groupby(\"timestamp\", as_index=False).agg(agg_map)\n",
    "    hourly[\"total_admits\"] = hourly[\"admits_in_site\"]\n",
    "    hourly[\"avg_duration_min\"] = hourly[\"dur_x_admits\"].div(hourly[\"admits_in_show\"]).round(1).fillna(0)\n",
    "\n",
    "    hourly = hourly.drop(columns=[\"admits_in_site\", \"admits_in_show\", \"dur_x_admits\", \"in_site\", \"in_show\"], errors=\"ignore\")\n",
    "\n",
    "    front = [\"timestamp\", \"total_admits\", \"avg_duration_min\"]\n",
    "    hourly = hourly[front + [c for c in hourly.columns if c not in front]]\n",
    "\n",
    "    output_path = output_dir / \"ohe_movie_sessions_hourly_expanded.xlsx\"\n",
    "    hourly.to_excel(output_path, index=False)\n",
    "    print(f\"✓ Saved hourly-expanded sessions → {output_path.name}  ({len(hourly)} rows)\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def merge_final_dataset(trx_ohe_path: Path, sess_ohe_path: Path, output_dir: Path, output_filename: str) -> None:\n",
    "    \"\"\"Merges OHE transaction and session data to create the final analysis dataset.\"\"\"\n",
    "    print(\"\\n--- Stage 5: Merging to Final Dataset ---\")\n",
    "    if not (trx_ohe_path and trx_ohe_path.exists()):\n",
    "        print(f\"⚠️ Transaction OHE file not found in '{output_dir}/'. Skipping merge.\")\n",
    "        return\n",
    "    if not (sess_ohe_path and sess_ohe_path.exists()):\n",
    "        print(f\"⚠️ Session OHE file not found in '{output_dir}/'. Skipping merge.\")\n",
    "        return\n",
    "\n",
    "    trx_df = pd.read_excel(trx_ohe_path, index_col=0, parse_dates=[0]).reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    sess_df = pd.read_excel(sess_ohe_path, parse_dates=[\"timestamp\"])\n",
    "\n",
    "    if trx_df[\"timestamp\"].duplicated().any(): raise ValueError(\"Transactions file has duplicate timestamps.\")\n",
    "    if sess_df[\"timestamp\"].duplicated().any(): raise ValueError(\"Sessions file has duplicate timestamps.\")\n",
    "\n",
    "    merged = pd.merge(trx_df, sess_df, on=\"timestamp\", how=\"left\", validate=\"one_to_one\")\n",
    "    print(f\"✓ Merged table size: {len(merged):,} rows × {merged.shape[1]} columns\")\n",
    "\n",
    "    lead = [\"timestamp\", \"total_admits\", \"total_price_aud\", \"avg_duration_min\"]\n",
    "    lead_existing = [c for c in lead if c in merged.columns]\n",
    "    merged = merged[lead_existing + [c for c in merged.columns if c not in lead_existing]]\n",
    "\n",
    "    final_output_path = output_dir / output_filename\n",
    "    merged.to_excel(final_output_path, index=False)\n",
    "    print(f\"• Saved final dataset to: {final_output_path}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# --- 5. MAIN PIPELINE WORKFLOW ---\n",
    "# =============================================================================\n",
    "\n",
    "def run_pipeline(output_filename: str):\n",
    "    \"\"\"\n",
    "    Executes the full data processing pipeline from raw files to the final dataset.\n",
    "\n",
    "    Args:\n",
    "        output_filename: The name of the final merged dataset file\n",
    "                         (e.g., \"train_dataset.xlsx\" or \"test_dataset.xlsx\").\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Data Cleanup Pipeline for '{output_filename}' ---\\n\")\n",
    "\n",
    "    # Ensure input/output dirs exist\n",
    "    INPUT_DIR.mkdir(exist_ok=True)\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Stage 1\n",
    "    cleaned_inventory_path = clean_inventory_transactions(INPUT_DIR, OUTPUT_DIR)\n",
    "\n",
    "    # Stage 2\n",
    "    one_hot_encode_transactions(cleaned_inventory_path, OUTPUT_DIR)\n",
    "\n",
    "    # Stage 3\n",
    "    cleaned_sessions_path = clean_movie_sessions(INPUT_DIR, OUTPUT_DIR)\n",
    "\n",
    "    # Stage 4\n",
    "    expanded_sessions_path = expand_and_ohe_sessions(cleaned_sessions_path, OUTPUT_DIR)\n",
    "\n",
    "    # Stage 5\n",
    "    trx_ohe_final_path = OUTPUT_DIR / \"ohe_trx_item_class_product.xlsx\"\n",
    "    merge_final_dataset(trx_ohe_final_path, expanded_sessions_path, OUTPUT_DIR, output_filename)\n",
    "\n",
    "    print(f\"\\n🎉 Pipeline complete. Final file '{output_filename}' is in '{OUTPUT_DIR}/'.\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- SCRIPT ENTRY POINT ---\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # This script can be run for either training or test data by changing the output filename.\n",
    "    # It automatically detects raw files in the 'input' folder and saves to 'output'.\n",
    "    current_folder_name = BASE_DIR.resolve().name.lower()\n",
    "\n",
    "    if \"train\" in current_folder_name:\n",
    "        final_output_file = \"train_dataset.xlsx\"\n",
    "    elif \"test\" in current_folder_name:\n",
    "        final_output_file = \"test_dataset.xlsx\"\n",
    "    else:\n",
    "        final_output_file = \"processed_dataset.xlsx\"\n",
    "        print(f\"⚠️ Warning: Could not determine if this is for train/test based on folder name. Defaulting output to '{final_output_file}'.\")\n",
    "\n",
    "    run_pipeline(output_filename=final_output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

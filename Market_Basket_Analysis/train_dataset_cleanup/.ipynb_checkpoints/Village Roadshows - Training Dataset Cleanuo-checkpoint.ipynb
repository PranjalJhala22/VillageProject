{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c6791d-e574-4972-8fe6-79ab8cc588a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†’ Cleaning Inventory Transaction Data 2023 v0.1.xlsx\n",
      "â†’ Cleaning Inventory Transaction Data 2024 v0.1.xlsx\n",
      "â†’ Cleaning Inventory Transaction Data Feb 2025 v1.xlsx\n",
      "â†’ Cleaning Inventory Transaction Data Jan 2025 v3.xlsx\n",
      "\n",
      "âœ… Cleaned rows total: 301,690\n",
      "  â€¢ inventory_transactions_clean.xlsx written\n",
      "  â€¢ inventory_transactions_clean_2023.xlsx  (120,965 rows)\n",
      "  â€¢ inventory_transactions_clean_2024.xlsx  (160,165 rows)\n",
      "  â€¢ inventory_transactions_clean_2025.xlsx  (20,560 rows)\n",
      "\n",
      "ğŸ‰ All inventory outputs generated â€“ ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "clean_inventory_transactions.py\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Consolidate and clean every workbook named\n",
    "    â€œInventory Transaction Data <YEAR> v0.1.xlsxâ€\n",
    "in the working directory.\n",
    "\n",
    "Main steps\n",
    "----------\n",
    "1. Normalise headers (ragged / merged rows in source files).\n",
    "2. Retain only allowed *item_class* values.\n",
    "3. Parse timestamps â†’ single `timestamp` column (date + hour).\n",
    "4. Impute missing / zero prices by a fiveâ€‘level hierarchy:\n",
    "      a. Exact product match\n",
    "      b. â€œNO ICE / NO SUGARâ€ proxy\n",
    "      c. Median unit price for *SNACKÂ -Â CHIPS*\n",
    "      d. Median unit price for the item_class\n",
    "      e. Global median unit price\n",
    "5. Reâ€‘calculate `price_aud` = `unit_price` Ã— `quantity`.\n",
    "6. Write:\n",
    "      â€¢ inventory_transactions_clean.xlsx         (all years combined)\n",
    "      â€¢ inventory_transactions_clean_<YEAR>.xlsx  (one per year)\n",
    "\n",
    "The resulting files have **exactly** these columns:\n",
    "    timestamp | item_class | product_name | quantity | price_aud\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "from typing import Final, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CONFIG\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATA_DIR: Final[Path] = Path(\".\")\n",
    "WORKBOOK_RX: Final[re.Pattern[str]] = re.compile(\n",
    "    r\"Inventory Transaction Data \"      # Starts with this exact phrase\n",
    "    r\"(?:\"                               # Start of non-capturing group for month/year variations\n",
    "        r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}\"  # Optional: Month Name + space + Year (e.g., Feb 2025)\n",
    "        r\"|\"                                # OR\n",
    "        r\"\\d{4}\"                           # Just the Year (e.g., 2023, 2024)\n",
    "    r\")\"                                 # End of non-capturing group\n",
    "    r\"(?:\\s+v\\d+(?:\\.\\d+)?)?\"            # Optional: space + version (e.g., \" v0.1\", \" v1\")\n",
    "    r\"\\.xlsx$\",                          # Ends with .xlsx\n",
    "    re.I                                 # Case-insensitive\n",
    ")\n",
    "SHEET_NAME: Final[str] = \"Inventory Trans\"\n",
    "\n",
    "ALLOWED_CLASSES: Final[set[str]] = {\n",
    "    \"SNACK - CHIPS\",\n",
    "    \"FOOD - VJUNIOR\",\n",
    "    \"ICE CREAMS - OTHER\",\n",
    "    \"ICE CREAMS - CHOC TO\",\n",
    "    \"DRINKS - EXTRA LARGE\",\n",
    "    \"DRINKS - LARGE\",\n",
    "    \"DRINKS - MEDIUM\",\n",
    "    \"DRINKS - SMALL\",\n",
    "    \"DRINKS - NO ICE\",\n",
    "    \"DRINKS\",\n",
    "    \"POPCORN\",\n",
    "}\n",
    "\n",
    "OUTPUT_COLS: Final[list[str]] = [\n",
    "    \"timestamp\",\n",
    "    \"item_class\",\n",
    "    \"product_name\",\n",
    "    \"quantity\",\n",
    "    \"price_aud\",\n",
    "]\n",
    "\n",
    "NO_FLAG_RX: Final[re.Pattern[str]] = re.compile(r\"\\bNO\\s+(ICE|SUGAR)\\b\", re.I)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# UTILITY FUNCTIONS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _find_header_row(xl_path: Path) -> int:\n",
    "    \"\"\"Locate the header row that contains 'Transaction Date'.\"\"\"\n",
    "    raw = pd.read_excel(xl_path, sheet_name=SHEET_NAME, header=None, dtype=str)\n",
    "    for idx, row in raw.iterrows():\n",
    "        if any(str(cell).lower().startswith(\"transaction date\") for cell in row):\n",
    "            return idx\n",
    "    raise ValueError(f\"{xl_path.name}: header row not found\")\n",
    "\n",
    "\n",
    "def _remove_no_flag(text: str) -> str:\n",
    "    \"\"\"Strip 'NO ICE' / 'NO SUGAR' tokens â†’ base product name.\"\"\"\n",
    "    return NO_FLAG_RX.sub(\"\", str(text)).replace(\"  \", \" \").strip()\n",
    "\n",
    "\n",
    "def _load_and_clean_file(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Return a cleaned DataFrame for one source workbook.\"\"\"\n",
    "    print(f\"â†’ Cleaning {xl_path.name}\")\n",
    "    hdr_row = _find_header_row(xl_path)\n",
    "    df_raw = pd.read_excel(xl_path, sheet_name=SHEET_NAME, header=hdr_row, dtype=str)\n",
    "\n",
    "    # Drop summary/footer rows that contain the word 'result'\n",
    "    df = df_raw[~df_raw[\"Transaction Date\"].str.contains(\"result\", case=False, na=False)]\n",
    "\n",
    "    # Rename essential columns, drop 'Unnamed' junk columns\n",
    "    rename_map: dict[str, str] = {}\n",
    "    drop_cols = [c for c in df.columns if c.lower().startswith(\"unnamed\")]\n",
    "    for col in df.columns:\n",
    "        key = col.lower()\n",
    "        if \"no of items\" in key or key.strip() == \"ea\":\n",
    "            rename_map[col] = \"quantity\"\n",
    "        elif \"sell price\" in key or key.strip() == \"aud\":\n",
    "            rename_map[col] = \"price_aud\"\n",
    "    df = df.drop(columns=drop_cols, errors=\"ignore\").rename(columns=rename_map)\n",
    "\n",
    "    # Parse dates / hours\n",
    "    df[\"Transaction Date\"] = pd.to_datetime(df[\"Transaction Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    df[\"Transaction Hour\"] = pd.to_numeric(df[\"Transaction Hour\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Transaction Date\", \"Transaction Hour\"])\n",
    "    df[\"Transaction Hour\"] = df[\"Transaction Hour\"].astype(int)\n",
    "\n",
    "    # Whitespace normalisation\n",
    "    for col in df.select_dtypes(include=\"object\"):\n",
    "        df[col] = df[col].str.strip()\n",
    "    df[\"Item Class\"] = df[\"Item Class\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    # Ensure numeric quantity / price\n",
    "    df[\"quantity\"] = (\n",
    "        pd.to_numeric(df[\"quantity\"], errors=\"coerce\")\n",
    "          .fillna(1)\n",
    "          .clip(lower=1)\n",
    "          .astype(int)\n",
    "    )\n",
    "    df[\"price_aud\"] = pd.to_numeric(df[\"price_aud\"], errors=\"coerce\")\n",
    "\n",
    "    # Build timestamp\n",
    "    df[\"timestamp\"] = (\n",
    "        df[\"Transaction Date\"].dt.normalize()\n",
    "        + pd.to_timedelta(df[\"Transaction Hour\"], unit=\"h\")\n",
    "    )\n",
    "\n",
    "    # Keep only allowed item classes\n",
    "    df = df[df[\"Item Class\"].str.upper().isin(ALLOWED_CLASSES)]\n",
    "\n",
    "    # â”€â”€ Price imputation hierarchy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df[\"unit_price\"] = df[\"price_aud\"] / df[\"quantity\"]\n",
    "\n",
    "    # 1) exact product match map\n",
    "    unit_price_map = (\n",
    "        df[df[\"unit_price\"] > 0][[\"VISTA Item\", \"unit_price\"]]\n",
    "          .drop_duplicates(\"VISTA Item\")\n",
    "          .set_index(\"VISTA Item\")[\"unit_price\"]\n",
    "    )\n",
    "\n",
    "    # Helper: fill missing unit_price from a mapping\n",
    "    def _fill_from_map(mask: pd.Series, key_series: pd.Series, price_map: pd.Series):\n",
    "        df.loc[mask, \"unit_price\"] = key_series[mask].map(price_map)\n",
    "\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] == 0)\n",
    "    _fill_from_map(missing_mask, df[\"VISTA Item\"], unit_price_map)\n",
    "\n",
    "    # 2) NO ICE / NO SUGAR proxy\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] == 0)\n",
    "    if missing_mask.any():\n",
    "        proxy_map = {k: v for k, v in unit_price_map.items()}\n",
    "        df.loc[missing_mask, \"unit_price\"] = (\n",
    "            df.loc[missing_mask, \"VISTA Item\"].apply(_remove_no_flag).map(proxy_map)\n",
    "        )\n",
    "\n",
    "    # 3) SNACK - CHIPS median\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] == 0)\n",
    "    chips_mask = missing_mask & (df[\"Item Class\"] == \"SNACK - CHIPS\")\n",
    "    if chips_mask.any():\n",
    "        chips_median = df.loc[df[\"Item Class\"] == \"SNACK - CHIPS\", \"unit_price\"].median()\n",
    "        df.loc[chips_mask, \"unit_price\"] = chips_median\n",
    "\n",
    "    # 4) itemâ€‘class median\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] == 0)\n",
    "    if missing_mask.any():\n",
    "        class_medians = (\n",
    "            df[df[\"unit_price\"] > 0]\n",
    "              .groupby(\"Item Class\")[\"unit_price\"]\n",
    "              .median()\n",
    "        )\n",
    "        df.loc[missing_mask, \"unit_price\"] = df.loc[missing_mask, \"Item Class\"].map(class_medians)\n",
    "\n",
    "    # 5) global median\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] == 0)\n",
    "    if missing_mask.any():\n",
    "        df.loc[missing_mask, \"unit_price\"] = df[\"unit_price\"].median(skipna=True)\n",
    "\n",
    "    # Rebuild total row price\n",
    "    df[\"price_aud\"] = (df[\"unit_price\"] * df[\"quantity\"]).round(2)\n",
    "    df = df.drop(columns=\"unit_price\")\n",
    "\n",
    "    # Warn if any rows still missing price\n",
    "    still_missing = df[\"price_aud\"].isna().sum()\n",
    "    if still_missing:\n",
    "        warnings.warn(f\"{xl_path.name}: {still_missing} rows still lack price.\")\n",
    "\n",
    "    # Final tidy\n",
    "    return (\n",
    "        df.rename(columns={\n",
    "            \"Item Class\": \"item_class\",\n",
    "            \"VISTA Item\": \"product_name\",\n",
    "        })[OUTPUT_COLS]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# MAIN\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main() -> None:\n",
    "    source_files: List[Path] = sorted(p for p in DATA_DIR.iterdir() if WORKBOOK_RX.fullmatch(p.name))\n",
    "    if not source_files:\n",
    "        print(\"ğŸ“­ No source workbooks found â€“ script did nothing.\")\n",
    "        return\n",
    "\n",
    "    cleaned_frames = [_load_and_clean_file(p) for p in source_files]\n",
    "    master = (\n",
    "        pd.concat(cleaned_frames, ignore_index=True)\n",
    "          .sort_values(\"timestamp\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"\\nâœ… Cleaned rows total: {len(master):,}\")\n",
    "\n",
    "    # combined file\n",
    "    master.to_excel(\"inventory_transactions_clean.xlsx\", index=False)\n",
    "    print(\"  â€¢ inventory_transactions_clean.xlsx written\")\n",
    "\n",
    "    # perâ€‘year splits\n",
    "    for year, group in master.groupby(master[\"timestamp\"].dt.year, sort=True):\n",
    "        fname = f\"inventory_transactions_clean_{year}.xlsx\"\n",
    "        group.to_excel(fname, index=False)\n",
    "        print(f\"  â€¢ {fname}  ({len(group):,} rows)\")\n",
    "\n",
    "    print(\"\\nğŸ‰ All inventory outputs generated â€“ ready for analysis.\")\n",
    "\n",
    "# Run in notebook or as script\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b4d8bf-166d-4a81-8f13-0cb9ea76701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OHE transaction files written:\n",
      "  â€¢ ohe_trx_item_class.xlsx\n",
      "  â€¢ ohe_trx_product_name.xlsx\n",
      "  â€¢ ohe_trx_item_class_product.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ohe_transactions_with_revenue.ipynb\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Build three transaction-level one-hot-encoded (OHE) matrices from\n",
    "# `inventory_transactions_clean.xlsx`, prepend an hourly-revenue column\n",
    "# (`total_price_aud`, 2-decimal float), and write each matrix to Excel.\n",
    "# --------------------------------------------------------------------------\n",
    "# Output files (all indexed by *timestamp*):\n",
    "#   â€¢ ohe_trx_item_class.xlsx\n",
    "#   â€¢ ohe_trx_product_name.xlsx\n",
    "#   â€¢ ohe_trx_item_class_product.xlsx\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "SRC_FILE: Path = Path(\"inventory_transactions_clean.xlsx\")\n",
    "OUTPUT_DIR: Path = Path(\".\")\n",
    "\n",
    "FILE_ITEM_CLASS   = OUTPUT_DIR / \"ohe_trx_item_class.xlsx\"\n",
    "FILE_PRODUCT_NAME = OUTPUT_DIR / \"ohe_trx_product_name.xlsx\"\n",
    "FILE_COMBINED     = OUTPUT_DIR / \"ohe_trx_item_class_product.xlsx\"\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# HELPER FUNCTIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def build_ohe_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    cat_cols: list[str],\n",
    "    *,\n",
    "    keep_prefix: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a timestamp-indexed OHE matrix for `cat_cols`, weighted by true\n",
    "    `quantity` per row.\n",
    "\n",
    "    Steps:\n",
    "      1. One-hot-encode JUST the categorical columns â†’ dummy_df\n",
    "      2. Multiply each dummy column by df['quantity']\n",
    "      3. Group by df['timestamp'] and sum â†’ true counts per hour\n",
    "    \"\"\"\n",
    "    if \"quantity\" not in df.columns:\n",
    "        raise KeyError(\"Must have a 'quantity' column to count units.\")\n",
    "\n",
    "    # 1) one-hot JUST the categories\n",
    "    dummy_kwargs = {} if keep_prefix else dict(prefix=\"\", prefix_sep=\"\")\n",
    "    dummies = pd.get_dummies(\n",
    "        df[cat_cols],\n",
    "        columns=cat_cols,\n",
    "        dtype=\"uint32\",\n",
    "        **dummy_kwargs\n",
    "    )\n",
    "\n",
    "    # 2) weight by actual quantity sold\n",
    "    weighted = dummies.mul(df[\"quantity\"].values, axis=0)\n",
    "\n",
    "    # 3) roll up by timestamp\n",
    "    ohe = (\n",
    "        weighted\n",
    "        .groupby(df[\"timestamp\"], as_index=True)\n",
    "        .sum()\n",
    "        .astype(\"uint32\")\n",
    "    )\n",
    "\n",
    "    return ohe\n",
    "\n",
    "\n",
    "def prepend_column(df: pd.DataFrame, name: str, data: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Insert `data` as the first column of `df`.\"\"\"\n",
    "    df.insert(0, name, data)\n",
    "    return df\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# LOAD TRANSACTIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "transactions = pd.read_excel(SRC_FILE, parse_dates=[\"timestamp\"])\n",
    "\n",
    "if transactions.isna().any().any():\n",
    "    warnings.warn(\"âš ï¸ Missing values in transaction file.\")\n",
    "\n",
    "# Standardise text\n",
    "transactions[\"item_class\"]   = transactions[\"item_class\"].str.upper().str.strip()\n",
    "transactions[\"product_name\"] = transactions[\"product_name\"].str.upper().str.strip()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# HOURLY REVENUE  (rounded to 2 decimals)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "hourly_revenue = (\n",
    "    transactions\n",
    "      .groupby(\"timestamp\")[\"price_aud\"]\n",
    "      .sum()\n",
    "      .round(2)\n",
    "      .rename(\"total_price_aud\")\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# BUILD OHE MATRICES (count-based)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "ohe_item_class = build_ohe_matrix(transactions, [\"item_class\"], keep_prefix=False)\n",
    "ohe_product    = build_ohe_matrix(transactions, [\"product_name\"], keep_prefix=False)\n",
    "ohe_combined   = build_ohe_matrix(\n",
    "    transactions, [\"item_class\",\"product_name\"], keep_prefix=True\n",
    ")\n",
    "\n",
    "# prepend revenue\n",
    "for mat in (ohe_item_class, ohe_product, ohe_combined):\n",
    "    prepend_column(mat, \"total_price_aud\", hourly_revenue)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SAVE TO EXCEL\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "ohe_item_class.to_excel(FILE_ITEM_CLASS)\n",
    "ohe_product.to_excel   (FILE_PRODUCT_NAME)\n",
    "ohe_combined.to_excel  (FILE_COMBINED)\n",
    "\n",
    "print(\"âœ“ OHE transaction files written:\")\n",
    "for p in (FILE_ITEM_CLASS, FILE_PRODUCT_NAME, FILE_COMBINED):\n",
    "    print(\"  â€¢\", p.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1729c01-a7c3-4f89-8d9c-cc83982c11a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Total cleaned sessions: 42,728\n",
      "â€¢ movie_sessions_clean.xlsx written\n",
      "â€¢ movie_sessions_clean_2023.xlsx (19,212 rows)\n",
      "â€¢ movie_sessions_clean_2024.xlsx (20,061 rows)\n",
      "â€¢ movie_sessions_clean_2025.xlsx (3,455 rows)\n",
      "\n",
      "ğŸ‰ Movieâ€‘session cleaning complete â€“ files ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "clean_movie_sessions.py\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Normalise and filter every workbook that matches the pattern:\n",
    "    Movie_sessions v<X.Y>.xlsx\n",
    "\n",
    "Business rules\n",
    "--------------\n",
    "â–ª Exclude sessions whose *Genre* is â€œGAMINGâ€ or â€œTOâ€¯BEâ€¯ADVISEDâ€.\n",
    "â–ª Remove records with placeholder runtime `duration_min == 960`.\n",
    "â–ª Keep only sessions that start between **09â€¯:â€¯00** and **21â€¯:â€¯59**.\n",
    "â–ª Categorise duration as *short / medium / long* and assign a\n",
    "  timeâ€‘ofâ€‘day *slot* (morning â†’ night_6).\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "movie_sessions_clean.xlsx                 â€”Â all years combined\n",
    "movie_sessions_clean_<YEAR>.xlsx          â€”Â one file per calendar year\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Final, Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# CONFIGURATION CONSTANTS\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "DATA_DIR: Final[Path]          = Path(\".\")\n",
    "SOURCE_RX: Final[re.Pattern[str]] = re.compile(\n",
    "    r\"Movie_sessions\"                    # Starts with \"Movie_sessions\"\n",
    "    r\"(?:_\"                               # Optional non-capturing group for month/year part\n",
    "        r\"(?:(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s?\\d{4})\"  # Optional: Underscore, Month Name, optional space, Year (e.g., _Feb 2025, _Jan2025)\n",
    "    r\")?\"                                # Make this whole month/year part optional\n",
    "    r\"(?:\\s+v\\d+(?:\\.\\d+)?)?\"            # Optional: space + version (e.g., \" v0.1\", \" v1\")\n",
    "    r\"\\.xlsx$\",                          # Ends with .xlsx\n",
    "    re.I                                 # Case-insensitive\n",
    ")\n",
    "SHEET_NAME: Final[str]         = \"Sheet1\"\n",
    "\n",
    "DROP_COLUMNS: Final[set[str]]  = {\"Film\"}\n",
    "EXCLUDE_GENRES: Final[set[str]] = {\"GAMING\", \"TO BE ADVISED\"}\n",
    "\n",
    "# Cinema tradingâ€‘hour slots (minutes after midnight: [start, end) )\n",
    "SLOT_WINDOWS: Final[Dict[str, Tuple[int, int]]] = {\n",
    "    \"morning\":     ( 9*60, 11*60),\n",
    "    \"early_noon\":  (11*60, 13*60),\n",
    "    \"noon\":        (13*60, 15*60),\n",
    "    \"late_noon\":   (15*60, 17*60),\n",
    "    \"evening_1\":   (17*60, 17*60 + 30),\n",
    "    \"evening_2\":   (17*60 + 30, 18*60),\n",
    "    \"evening_3\":   (18*60, 18*60 + 15),\n",
    "    \"evening_4\":   (18*60 + 15, 18*60 + 30),\n",
    "    \"evening_5\":   (18*60 + 30, 18*60 + 45),\n",
    "    \"evening_6\":   (18*60 + 45, 19*60),\n",
    "    \"night_1\":     (19*60, 19*60 + 15),\n",
    "    \"night_2\":     (19*60 + 15, 19*60 + 30),\n",
    "    \"night_3\":     (19*60 + 30, 20*60),\n",
    "    \"night_4\":     (20*60, 20*60 + 30),\n",
    "    \"night_5\":     (20*60 + 30, 21*60),\n",
    "    \"night_6\":     (21*60, 22*60),   # 21:00Â â€“Â 21:59\n",
    "}\n",
    "\n",
    "# Duration categories (minutes)\n",
    "SHORT_MAX:  Final[int] = 120\n",
    "MEDIUM_MAX: Final[int] = 160\n",
    "\n",
    "CLEAN_COLS: Final[list[str]] = [\n",
    "    \"timestamp\", \"language\", \"genre\", \"rating\",\n",
    "    \"admits\", \"duration_min\", \"duration_category\", \"slot\",\n",
    "]\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# HELPER FUNCTIONS\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def _locate_header_row(xl_path: Path) -> int:\n",
    "    \"\"\"Return the row number containing 'Session Date' (header row).\"\"\"\n",
    "    raw = pd.read_excel(xl_path, sheet_name=SHEET_NAME, header=None, dtype=str)\n",
    "    for idx, row in raw.iterrows():\n",
    "        if any(str(cell).lower().startswith(\"session date\") for cell in row):\n",
    "            return idx\n",
    "    raise ValueError(f\"{xl_path.name}: header row not found\")\n",
    "\n",
    "\n",
    "def _slot_from_timestamp(ts: pd.Timestamp) -> str:\n",
    "    \"\"\"Map a timestamp to its cinema time slot, or 'out_of_range'.\"\"\"\n",
    "    minutes = ts.hour * 60 + ts.minute\n",
    "    for slot, (start, end) in SLOT_WINDOWS.items():\n",
    "        if start <= minutes < end:\n",
    "            return slot\n",
    "    return \"out_of_range\"\n",
    "\n",
    "\n",
    "def _duration_bucket(minutes: int) -> str:\n",
    "    if minutes <= SHORT_MAX:\n",
    "        return \"short\"\n",
    "    if minutes <= MEDIUM_MAX:\n",
    "        return \"medium\"\n",
    "    return \"long\"\n",
    "\n",
    "\n",
    "def _parse_duration(duration_text: str) -> int:\n",
    "    match = re.search(r\"(\\d+)\", str(duration_text))\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# CORE CLEANING ROUTINE\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def clean_one_workbook(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load â†’ clean â†’ return a standardised DataFrame for one workbook.\"\"\"\n",
    "    header_row = _locate_header_row(xl_path)\n",
    "    df = pd.read_excel(xl_path, sheet_name=SHEET_NAME, header=header_row, dtype=str)\n",
    "\n",
    "    # Drop columns we never use (e.g., 'Film')\n",
    "    df = df.drop(columns=DROP_COLUMNS, errors=\"ignore\")\n",
    "\n",
    "    # Parse and validate date / hour columns\n",
    "    df[\"Session Date\"] = pd.to_datetime(df[\"Session Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    df[\"Session Hour\"] = pd.to_numeric(df[\"Session Hour\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Session Date\", \"Session Hour\", \"Genre\", \"Total Admits\", \"Duration\"])\n",
    "    df[\"Session Hour\"] = df[\"Session Hour\"].astype(int)\n",
    "\n",
    "    # Filter out unwanted genres\n",
    "    df = df[~df[\"Genre\"].str.upper().str.strip().isin(EXCLUDE_GENRES)]\n",
    "\n",
    "    # Build timestamp\n",
    "    df[\"timestamp\"] = (\n",
    "        df[\"Session Date\"].dt.normalize() +\n",
    "        pd.to_timedelta(df[\"Session Hour\"], unit=\"h\")\n",
    "    )\n",
    "\n",
    "    # Parse runtime and drop placeholder 960â€‘min rows\n",
    "    df[\"duration_min\"] = df[\"Duration\"].apply(_parse_duration)\n",
    "    df = df[df[\"duration_min\"] != 960]\n",
    "    df[\"duration_category\"] = df[\"duration_min\"].apply(_duration_bucket)\n",
    "\n",
    "    # Assign cinema slot; keep only tradingâ€‘hour rows\n",
    "    df[\"slot\"] = df[\"timestamp\"].apply(_slot_from_timestamp)\n",
    "    df = df[df[\"slot\"] != \"out_of_range\"]\n",
    "\n",
    "    # Standardise column names\n",
    "    df = df.rename(columns={\n",
    "        \"Session Audio Language\": \"language\",\n",
    "        \"Genre\":                  \"genre\",\n",
    "        \"Censor Rating\":          \"rating\",\n",
    "        \"Total Admits\":           \"admits\",\n",
    "    })\n",
    "\n",
    "    return df[CLEAN_COLS].sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# MAIN PIPELINE\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def main() -> None:\n",
    "    workbooks = sorted(p for p in DATA_DIR.iterdir() if SOURCE_RX.fullmatch(p.name))\n",
    "    if not workbooks:\n",
    "        print(\"ğŸ“­ No source workbooks found â€“ nothing to clean.\")\n",
    "        return\n",
    "\n",
    "    frames = [clean_one_workbook(p) for p in workbooks]\n",
    "    combined = pd.concat(frames, ignore_index=True).sort_values(\"timestamp\")\n",
    "\n",
    "    print(f\"\\nâœ… Total cleaned sessions: {len(combined):,}\")\n",
    "\n",
    "    # Combined file\n",
    "    combined.to_excel(\"movie_sessions_clean.xlsx\", index=False)\n",
    "    print(\"â€¢ movie_sessions_clean.xlsx written\")\n",
    "\n",
    "    # Yearly splits\n",
    "    for year, grp in combined.groupby(combined[\"timestamp\"].dt.year, sort=True):\n",
    "        fname = f\"movie_sessions_clean_{year}.xlsx\"\n",
    "        grp.to_excel(fname, index=False)\n",
    "        print(f\"â€¢ {fname} ({len(grp):,} rows)\")\n",
    "\n",
    "    print(\"\\nğŸ‰ Movieâ€‘session cleaning complete â€“ files ready for analysis.\")\n",
    "\n",
    "\n",
    "# Run automatically in notebook / script\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9595038a-207b-4dee-837e-511c3d1b3822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved hourly-expanded sessions â†’ ohe_movie_sessions_hourly_expanded.xlsx  (11763 rows)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# make_ohe_movie_sessions_hourly_expanded.ipynb\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Create an **hourâ€‘level** feature table for cinema sessions that\n",
    "# reflects every hour patrons are onâ€‘site, not just the start time.\n",
    "#\n",
    "# Steps\n",
    "# -----\n",
    "# 1.  Expand each session to the hours it overlaps\n",
    "#     â€¢ optional `PRE_BUFFER_HRS`  (earlyâ€‘arrival window)\n",
    "#     â€¢ optional `POST_BUFFER_HRS` (linger window)\n",
    "# 2.  Oneâ€‘hotâ€‘encode categorical fields.\n",
    "# 3.  Aggregate to one row per `timestamp` and compute:\n",
    "#       â€“ total_admits\n",
    "#       â€“ avg_duration_min  (admitsâ€‘weighted runtime)\n",
    "#\n",
    "# Output\n",
    "# ------\n",
    "#   ohe_movie_sessions_hourly_expanded.xlsx\n",
    "#\n",
    "# Column order\n",
    "# ------------\n",
    "# timestamp | total_admits | avg_duration_min | <dummy columns â€¦>\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "from __future__ import annotations\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from typing import Final, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CONFIGURATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# How many hours before a session patrons might arrive\n",
    "PRE_BUFFER_HRS:  Final[int] = 1\n",
    "# How many hours after a session patrons might linger (rarely used)\n",
    "POST_BUFFER_HRS: Final[int] = 0\n",
    "\n",
    "# Path to the cleaned movie-session file and where to write the expanded OHE file\n",
    "PATH_SESS_CLEAN: Final[Path] = Path(\"movie_sessions_clean.xlsx\")\n",
    "OUT_SESS_EXP:    Final[Path] = Path(\"ohe_movie_sessions_hourly_expanded.xlsx\")\n",
    "\n",
    "# Column names\n",
    "TS_COL:   Final[str] = \"timestamp\"\n",
    "ADMITS:   Final[str] = \"total_admits\"\n",
    "AVG_DUR:  Final[str] = \"avg_duration_min\"\n",
    "\n",
    "# Which categorical columns to one-hot encode\n",
    "CAT_COLS: Final[List[str]] = [\n",
    "    \"language\",\n",
    "    \"genre\",\n",
    "    \"rating\",\n",
    "    \"slot\",\n",
    "    \"duration_category\",\n",
    "]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1) LOAD & EXPAND EACH SESSION INTO HOURLY â€œBINSâ€\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sessions = pd.read_excel(PATH_SESS_CLEAN, parse_dates=[TS_COL])\n",
    "\n",
    "# Compute when each screening ends\n",
    "sessions[\"end_time\"] = sessions[TS_COL] + pd.to_timedelta(\n",
    "    sessions[\"duration_min\"], unit=\"m\"\n",
    ")\n",
    "\n",
    "expanded_rows: list[pd.Series] = []\n",
    "\n",
    "for _, sess in sessions.iterrows():\n",
    "    real_start = sess[TS_COL]\n",
    "    real_end   = sess[\"end_time\"]\n",
    "\n",
    "    # Define the full window when patrons might be on site\n",
    "    site_start = real_start - timedelta(hours=PRE_BUFFER_HRS)\n",
    "    site_end   = real_end   + timedelta(hours=POST_BUFFER_HRS)\n",
    "\n",
    "    # For every hour mark in that window...\n",
    "    for hr in pd.date_range(site_start, site_end, freq=\"h\", inclusive=\"left\"):\n",
    "        row = sess.copy()\n",
    "        row[TS_COL]    = hr\n",
    "        # Flag whether this hour is within the site-visit window\n",
    "        row[\"in_site\"] = int(site_start <= hr < site_end)\n",
    "        # Flag whether this hour is during the actual show\n",
    "        row[\"in_show\"] = int(real_start <= hr < real_end)\n",
    "        expanded_rows.append(row)\n",
    "\n",
    "# Build expanded DataFrame and drop the temporary end_time\n",
    "expanded = pd.DataFrame(expanded_rows).drop(columns=[\"end_time\"])\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2) ONE-HOT-ENCODE CATEGORIES, ZERO-OUT WHEN NOT IN SHOW, TRACK ADMITS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ohe = pd.get_dummies(\n",
    "    expanded,\n",
    "    columns=CAT_COLS,\n",
    "    prefix=CAT_COLS,\n",
    "    prefix_sep=\"_\",\n",
    "    dtype=\"uint8\",\n",
    ")\n",
    "\n",
    "# Find all of our newly created dummy columns\n",
    "cat_dummies = [\n",
    "    c for c in ohe.columns\n",
    "    if any(c.startswith(f\"{cat}_\") for cat in CAT_COLS)\n",
    "]\n",
    "\n",
    "# Zero out category flags for any hour not actually in show\n",
    "ohe[cat_dummies] = ohe[cat_dummies].mul(ohe[\"in_show\"], axis=0)\n",
    "\n",
    "# Compute three key metrics per expanded row:\n",
    "#   1) admits_in_site = how many people are on site this hour\n",
    "#   2) admits_in_show = how many are in the actual screening this hour\n",
    "#   3) dur_x_admits   = duration * admits, but only counted when in_show\n",
    "ohe[\"admits_in_site\"] = ohe[\"admits\"] * ohe[\"in_site\"]\n",
    "ohe[\"admits_in_show\"] = ohe[\"admits\"] * ohe[\"in_show\"]\n",
    "ohe[\"dur_x_admits\"]   = ohe[\"duration_min\"] * ohe[\"admits\"] * ohe[\"in_show\"]\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3) AGGREGATE ALL ROWS TO HOUR-LEVEL\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Build aggregation mapping: sum all dummy flags & our three admits columns\n",
    "agg_map = {c: \"sum\" for c in cat_dummies}\n",
    "agg_map.update({\n",
    "    \"admits_in_site\": \"sum\",\n",
    "    \"admits_in_show\": \"sum\",\n",
    "    \"dur_x_admits\":   \"sum\",\n",
    "})\n",
    "\n",
    "hourly = (\n",
    "    ohe\n",
    "      .groupby(TS_COL, as_index=False)\n",
    "      .agg(agg_map)\n",
    ")\n",
    "\n",
    "# Now compute final columns:\n",
    "#  â€¢ total_admits   = admits_in_site\n",
    "#  â€¢ avg_duration   = (sum of dur_x_admits) / (sum of admits_in_show)\n",
    "hourly[ADMITS] = hourly[\"admits_in_site\"]\n",
    "hourly[AVG_DUR] = (\n",
    "    hourly[\"dur_x_admits\"]\n",
    "          .div(hourly[\"admits_in_show\"])\n",
    "          .round(1)\n",
    "          .fillna(0)\n",
    ")\n",
    "\n",
    "# Drop our intermediate columns\n",
    "hourly = hourly.drop(\n",
    "    columns=[\n",
    "        \"admits_in_site\",\n",
    "        \"admits_in_show\",\n",
    "        \"dur_x_admits\",\n",
    "        \"in_site\",\n",
    "        \"in_show\",\n",
    "    ],\n",
    "    errors=\"ignore\"\n",
    ")\n",
    "\n",
    "# Reorder so timestamp | total_admits | avg_duration_min | <dummiesâ€¦>\n",
    "front = [TS_COL, ADMITS, AVG_DUR]\n",
    "hourly = hourly[front + [c for c in hourly.columns if c not in front]]\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4) SAVE TO EXCEL\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "hourly.to_excel(OUT_SESS_EXP, index=False)\n",
    "print(f\"âœ“ Saved hourly-expanded sessions â†’ {OUT_SESS_EXP.name}  ({len(hourly)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6be61e0-8e12-4fa2-b81d-bc70d4cf0017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Merged table size: 10,040 rows Ã— 234 columns\n",
      "â€¢ Saved train_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# merge_trx_with_expanded_sessions.ipynb\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Combine hourlyâ€‘level **transaction** features with the hourlyâ€‘level,\n",
    "# bufferâ€‘expanded **session** features.\n",
    "#\n",
    "# Join logic\n",
    "# ----------\n",
    "# â€¢ Key      : timestamp  (YYYYâ€‘MMâ€‘DDÂ HH:00:00)\n",
    "# â€¢ Strategy : LEFT  (retain every hour that contains concession sales)\n",
    "#\n",
    "# Output\n",
    "# ------\n",
    "# merged_trx_sessions_expanded.xlsx\n",
    "#\n",
    "# Final column order\n",
    "# ------------------\n",
    "# timestamp | total_admits | total_price_aud | avg_duration_min | <dummiesâ€¦>\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# %%\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Final\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# CONFIGURATION CONSTANTS\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "PATH_TRX_OHE:   Final[Path] = Path(\"ohe_trx_item_class_product.xlsx\")\n",
    "PATH_SESS_OHE:  Final[Path] = Path(\"ohe_movie_sessions_hourly_expanded.xlsx\")\n",
    "PATH_OUT_MERGE: Final[Path] = Path(\"train_dataset.xlsx\")\n",
    "\n",
    "TS:        Final[str] = \"timestamp\"\n",
    "COL_PRICE: Final[str] = \"total_price_aud\"\n",
    "COL_ADMIT: Final[str] = \"total_admits\"\n",
    "COL_DUR:   Final[str] = \"avg_duration_min\"\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# LOADERS\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "def load_transactions(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load hourly transaction OHE.\n",
    "    Excel saved the timestamp index as the first unnamed column â†’ restore.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(xl_path, index_col=0, parse_dates=[0])\n",
    "    return df.reset_index().rename(columns={\"index\": TS})\n",
    "\n",
    "\n",
    "def load_sessions(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load hourly session OHE (timestamp already present).\"\"\"\n",
    "    return pd.read_excel(xl_path, parse_dates=[TS])\n",
    "\n",
    "\n",
    "def assert_unique_timestamps(df: pd.DataFrame, label: str) -> None:\n",
    "    \"\"\"Fail early if duplicates exist.\"\"\"\n",
    "    dupes = df[df[TS].duplicated()][TS].unique()\n",
    "    if dupes.size:\n",
    "        raise ValueError(f\"{label}: duplicate timestamps {dupes[:5]}â€¦\")\n",
    "\n",
    "\n",
    "def reorder_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Place key numeric columns first; keep original order otherwise.\"\"\"\n",
    "    lead = [TS, COL_ADMIT, COL_PRICE, COL_DUR]\n",
    "    lead = [c for c in lead if c in df.columns]  # tolerate missing cols\n",
    "    return df[lead + [c for c in df.columns if c not in lead]]\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# MAIN PIPELINE\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. Load sources\n",
    "trx_df  = load_transactions(PATH_TRX_OHE)\n",
    "sess_df = load_sessions(PATH_SESS_OHE)\n",
    "\n",
    "# 2. Basic sanity checks\n",
    "assert_unique_timestamps(trx_df,  \"Transactions\")\n",
    "assert_unique_timestamps(sess_df, \"Sessions\")\n",
    "\n",
    "# 3. LEFT join (keep every hour with sales)\n",
    "merged = pd.merge(\n",
    "    trx_df,\n",
    "    sess_df,\n",
    "    on=TS,\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_one\",\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Merged table size: {len(merged):,} rows Ã— {merged.shape[1]} columns\")\n",
    "\n",
    "# 4. Column ordering\n",
    "merged = reorder_columns(merged)\n",
    "\n",
    "# 5. Save\n",
    "merged.to_excel(PATH_OUT_MERGE, index=False)\n",
    "print(f\"â€¢ Saved {PATH_OUT_MERGE.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc47e61-d36f-4266-b018-74f3931e8086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c6791d-e574-4972-8fe6-79ab8cc588a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Cleaning Inventory Transaction Data 2023 v0.1.xlsx\n",
      "→ Cleaning Inventory Transaction Data 2024 v0.1.xlsx\n",
      "→ Cleaning Inventory Transaction Data Feb 2025 v1.xlsx\n",
      "→ Cleaning Inventory Transaction Data Jan 2025 v3.xlsx\n",
      "\n",
      "✅ Cleaned rows total: 301,690\n",
      "  • inventory_transactions_clean.xlsx written\n",
      "  • inventory_transactions_clean_2023.xlsx  (120,965 rows)\n",
      "  • inventory_transactions_clean_2024.xlsx  (160,165 rows)\n",
      "  • inventory_transactions_clean_2025.xlsx  (20,560 rows)\n",
      "\n",
      "🎉 All inventory outputs generated – ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "clean_inventory_transactions.py\n",
    "────────────────────────────────────────────────────────────────────\n",
    "Consolidate and clean every workbook named\n",
    "    “Inventory Transaction Data <YEAR> v0.1.xlsx”\n",
    "in the working directory.\n",
    "\n",
    "Main steps\n",
    "----------\n",
    "1. Normalise headers (ragged / merged rows in source files).\n",
    "2. Retain only allowed *item_class* values.\n",
    "3. Parse timestamps → single `timestamp` column (date + hour).\n",
    "4. Impute missing / zero prices by a five‑level hierarchy:\n",
    "      a. Exact product match\n",
    "      b. “NO ICE / NO SUGAR” proxy\n",
    "      c. Median unit price for *SNACK - CHIPS*\n",
    "      d. Median unit price for the item_class\n",
    "      e. Global median unit price\n",
    "5. Re‑calculate `price_aud` = `unit_price` × `quantity`.\n",
    "6. Write:\n",
    "      • inventory_transactions_clean.xlsx         (all years combined)\n",
    "      • inventory_transactions_clean_<YEAR>.xlsx  (one per year)\n",
    "\n",
    "The resulting files have **exactly** these columns:\n",
    "    timestamp | item_class | product_name | quantity | price_aud\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "from typing import Final, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# CONFIG\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "DATA_DIR: Final[Path] = Path(\".\")\n",
    "WORKBOOK_RX: Final[re.Pattern[str]] = re.compile(\n",
    "    r\"Inventory Transaction Data \"      # Starts with this exact phrase\n",
    "    r\"(?:\"                               # Start of non-capturing group for month/year variations\n",
    "        r\"(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}\"  # Optional: Month Name + space + Year (e.g., Feb 2025)\n",
    "        r\"|\"                                # OR\n",
    "        r\"\\d{4}\"                           # Just the Year (e.g., 2023, 2024)\n",
    "    r\")\"                                 # End of non-capturing group\n",
    "    r\"(?:\\s+v\\d+(?:\\.\\d+)?)?\"            # Optional: space + version (e.g., \" v0.1\", \" v1\")\n",
    "    r\"\\.xlsx$\",                          # Ends with .xlsx\n",
    "    re.I                                 # Case-insensitive\n",
    ")\n",
    "SHEET_NAME: Final[str] = \"Inventory Trans\"\n",
    "\n",
    "ALLOWED_CLASSES: Final[set[str]] = {\n",
    "    \"SNACK - CHIPS\",\n",
    "    \"FOOD - VJUNIOR\",\n",
    "    \"ICE CREAMS - OTHER\",\n",
    "    \"ICE CREAMS - CHOC TO\",\n",
    "    \"DRINKS - EXTRA LARGE\",\n",
    "    \"DRINKS - LARGE\",\n",
    "    \"DRINKS - MEDIUM\",\n",
    "    \"DRINKS - SMALL\",\n",
    "    \"DRINKS - NO ICE\",\n",
    "    \"DRINKS\",\n",
    "    \"POPCORN\",\n",
    "}\n",
    "\n",
    "OUTPUT_COLS: Final[list[str]] = [\n",
    "    \"timestamp\",\n",
    "    \"item_class\",\n",
    "    \"product_name\",\n",
    "    \"quantity\",\n",
    "    \"price_aud\",\n",
    "]\n",
    "\n",
    "NO_FLAG_RX: Final[re.Pattern[str]] = re.compile(r\"\\bNO\\s+(ICE|SUGAR)\\b\", re.I)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# UTILITY FUNCTIONS\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def _find_header_row(xl_path: Path) -> int:\n",
    "    \"\"\"Locate the header row that contains 'Transaction Date'.\"\"\"\n",
    "    raw = pd.read_excel(xl_path, sheet_name=SHEET_NAME, header=None, dtype=str)\n",
    "    for idx, row in raw.iterrows():\n",
    "        if any(str(cell).lower().startswith(\"transaction date\") for cell in row):\n",
    "            return idx\n",
    "    raise ValueError(f\"{xl_path.name}: header row not found\")\n",
    "\n",
    "\n",
    "def _remove_no_flag(text: str) -> str:\n",
    "    \"\"\"Strip 'NO ICE' / 'NO SUGAR' tokens → base product name.\"\"\"\n",
    "    return NO_FLAG_RX.sub(\"\", str(text)).replace(\"  \", \" \").strip()\n",
    "\n",
    "\n",
    "def _load_and_clean_file(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Return a cleaned DataFrame for one source workbook.\"\"\"\n",
    "    print(f\"→ Cleaning {xl_path.name}\")\n",
    "    hdr_row = _find_header_row(xl_path)\n",
    "    df_raw = pd.read_excel(xl_path, sheet_name=SHEET_NAME, header=hdr_row, dtype=str)\n",
    "\n",
    "    # Drop summary/footer rows that contain the word 'result'\n",
    "    df = df_raw[~df_raw[\"Transaction Date\"].str.contains(\"result\", case=False, na=False)]\n",
    "\n",
    "    # Rename essential columns, drop 'Unnamed' junk columns\n",
    "    rename_map: dict[str, str] = {}\n",
    "    drop_cols = [c for c in df.columns if c.lower().startswith(\"unnamed\")]\n",
    "    for col in df.columns:\n",
    "        key = col.lower()\n",
    "        if \"no of items\" in key or key.strip() == \"ea\":\n",
    "            rename_map[col] = \"quantity\"\n",
    "        elif \"sell price\" in key or key.strip() == \"aud\":\n",
    "            rename_map[col] = \"price_aud\"\n",
    "    df = df.drop(columns=drop_cols, errors=\"ignore\").rename(columns=rename_map)\n",
    "\n",
    "    # Parse dates / hours\n",
    "    df[\"Transaction Date\"] = pd.to_datetime(df[\"Transaction Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    df[\"Transaction Hour\"] = pd.to_numeric(df[\"Transaction Hour\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Transaction Date\", \"Transaction Hour\"])\n",
    "    df[\"Transaction Hour\"] = df[\"Transaction Hour\"].astype(int)\n",
    "\n",
    "    # Whitespace normalisation\n",
    "    for col in df.select_dtypes(include=\"object\"):\n",
    "        df[col] = df[col].str.strip()\n",
    "    df[\"Item Class\"] = df[\"Item Class\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    # Ensure numeric quantity / price\n",
    "    df[\"quantity\"] = (\n",
    "        pd.to_numeric(df[\"quantity\"], errors=\"coerce\")\n",
    "          .fillna(1)\n",
    "          .clip(lower=1)\n",
    "          .astype(int)\n",
    "    )\n",
    "    df[\"price_aud\"] = pd.to_numeric(df[\"price_aud\"], errors=\"coerce\")\n",
    "\n",
    "    # Build timestamp\n",
    "    df[\"timestamp\"] = (\n",
    "        df[\"Transaction Date\"].dt.normalize()\n",
    "        + pd.to_timedelta(df[\"Transaction Hour\"], unit=\"h\")\n",
    "    )\n",
    "\n",
    "    # Keep only allowed item classes\n",
    "    df = df[df[\"Item Class\"].str.upper().isin(ALLOWED_CLASSES)]\n",
    "\n",
    "    # ── Price imputation hierarchy ───────────────────────────────\n",
    "    df[\"unit_price\"] = df[\"price_aud\"] / df[\"quantity\"]\n",
    "\n",
    "    # 1) exact product match map\n",
    "    unit_price_map = (\n",
    "        df[df[\"unit_price\"] > 0][[\"VISTA Item\", \"unit_price\"]]\n",
    "          .drop_duplicates(\"VISTA Item\")\n",
    "          .set_index(\"VISTA Item\")[\"unit_price\"]\n",
    "    )\n",
    "\n",
    "    # Helper: fill missing unit_price from a mapping\n",
    "    def _fill_from_map(mask: pd.Series, key_series: pd.Series, price_map: pd.Series):\n",
    "        df.loc[mask, \"unit_price\"] = key_series[mask].map(price_map)\n",
    "\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] == 0)\n",
    "    _fill_from_map(missing_mask, df[\"VISTA Item\"], unit_price_map)\n",
    "\n",
    "    # 2) NO ICE / NO SUGAR proxy\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] == 0)\n",
    "    if missing_mask.any():\n",
    "        proxy_map = {k: v for k, v in unit_price_map.items()}\n",
    "        df.loc[missing_mask, \"unit_price\"] = (\n",
    "            df.loc[missing_mask, \"VISTA Item\"].apply(_remove_no_flag).map(proxy_map)\n",
    "        )\n",
    "\n",
    "    # 3) SNACK - CHIPS median\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] == 0)\n",
    "    chips_mask = missing_mask & (df[\"Item Class\"] == \"SNACK - CHIPS\")\n",
    "    if chips_mask.any():\n",
    "        chips_median = df.loc[df[\"Item Class\"] == \"SNACK - CHIPS\", \"unit_price\"].median()\n",
    "        df.loc[chips_mask, \"unit_price\"] = chips_median\n",
    "\n",
    "    # 4) item‑class median\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] == 0)\n",
    "    if missing_mask.any():\n",
    "        class_medians = (\n",
    "            df[df[\"unit_price\"] > 0]\n",
    "              .groupby(\"Item Class\")[\"unit_price\"]\n",
    "              .median()\n",
    "        )\n",
    "        df.loc[missing_mask, \"unit_price\"] = df.loc[missing_mask, \"Item Class\"].map(class_medians)\n",
    "\n",
    "    # 5) global median\n",
    "    missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] == 0)\n",
    "    if missing_mask.any():\n",
    "        df.loc[missing_mask, \"unit_price\"] = df[\"unit_price\"].median(skipna=True)\n",
    "\n",
    "    # Rebuild total row price\n",
    "    df[\"price_aud\"] = (df[\"unit_price\"] * df[\"quantity\"]).round(2)\n",
    "    df = df.drop(columns=\"unit_price\")\n",
    "\n",
    "    # Warn if any rows still missing price\n",
    "    still_missing = df[\"price_aud\"].isna().sum()\n",
    "    if still_missing:\n",
    "        warnings.warn(f\"{xl_path.name}: {still_missing} rows still lack price.\")\n",
    "\n",
    "    # Final tidy\n",
    "    return (\n",
    "        df.rename(columns={\n",
    "            \"Item Class\": \"item_class\",\n",
    "            \"VISTA Item\": \"product_name\",\n",
    "        })[OUTPUT_COLS]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# MAIN\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def main() -> None:\n",
    "    source_files: List[Path] = sorted(p for p in DATA_DIR.iterdir() if WORKBOOK_RX.fullmatch(p.name))\n",
    "    if not source_files:\n",
    "        print(\"📭 No source workbooks found – script did nothing.\")\n",
    "        return\n",
    "\n",
    "    cleaned_frames = [_load_and_clean_file(p) for p in source_files]\n",
    "    master = (\n",
    "        pd.concat(cleaned_frames, ignore_index=True)\n",
    "          .sort_values(\"timestamp\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"\\n✅ Cleaned rows total: {len(master):,}\")\n",
    "\n",
    "    # combined file\n",
    "    master.to_excel(\"inventory_transactions_clean.xlsx\", index=False)\n",
    "    print(\"  • inventory_transactions_clean.xlsx written\")\n",
    "\n",
    "    # per‑year splits\n",
    "    for year, group in master.groupby(master[\"timestamp\"].dt.year, sort=True):\n",
    "        fname = f\"inventory_transactions_clean_{year}.xlsx\"\n",
    "        group.to_excel(fname, index=False)\n",
    "        print(f\"  • {fname}  ({len(group):,} rows)\")\n",
    "\n",
    "    print(\"\\n🎉 All inventory outputs generated – ready for analysis.\")\n",
    "\n",
    "# Run in notebook or as script\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b4d8bf-166d-4a81-8f13-0cb9ea76701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OHE transaction files written:\n",
      "  • ohe_trx_item_class.xlsx\n",
      "  • ohe_trx_product_name.xlsx\n",
      "  • ohe_trx_item_class_product.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ohe_transactions_with_revenue.ipynb\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# Build three transaction-level one-hot-encoded (OHE) matrices from\n",
    "# `inventory_transactions_clean.xlsx`, prepend an hourly-revenue column\n",
    "# (`total_price_aud`, 2-decimal float), and write each matrix to Excel.\n",
    "# --------------------------------------------------------------------------\n",
    "# Output files (all indexed by *timestamp*):\n",
    "#   • ohe_trx_item_class.xlsx\n",
    "#   • ohe_trx_product_name.xlsx\n",
    "#   • ohe_trx_item_class_product.xlsx\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "SRC_FILE: Path = Path(\"inventory_transactions_clean.xlsx\")\n",
    "OUTPUT_DIR: Path = Path(\".\")\n",
    "\n",
    "FILE_ITEM_CLASS   = OUTPUT_DIR / \"ohe_trx_item_class.xlsx\"\n",
    "FILE_PRODUCT_NAME = OUTPUT_DIR / \"ohe_trx_product_name.xlsx\"\n",
    "FILE_COMBINED     = OUTPUT_DIR / \"ohe_trx_item_class_product.xlsx\"\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# HELPER FUNCTIONS\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "def build_ohe_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    cat_cols: list[str],\n",
    "    *,\n",
    "    keep_prefix: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a timestamp-indexed OHE matrix for `cat_cols`, weighted by true\n",
    "    `quantity` per row.\n",
    "\n",
    "    Steps:\n",
    "      1. One-hot-encode JUST the categorical columns → dummy_df\n",
    "      2. Multiply each dummy column by df['quantity']\n",
    "      3. Group by df['timestamp'] and sum → true counts per hour\n",
    "    \"\"\"\n",
    "    if \"quantity\" not in df.columns:\n",
    "        raise KeyError(\"Must have a 'quantity' column to count units.\")\n",
    "\n",
    "    # 1) one-hot JUST the categories\n",
    "    dummy_kwargs = {} if keep_prefix else dict(prefix=\"\", prefix_sep=\"\")\n",
    "    dummies = pd.get_dummies(\n",
    "        df[cat_cols],\n",
    "        columns=cat_cols,\n",
    "        dtype=\"uint32\",\n",
    "        **dummy_kwargs\n",
    "    )\n",
    "\n",
    "    # 2) weight by actual quantity sold\n",
    "    weighted = dummies.mul(df[\"quantity\"].values, axis=0)\n",
    "\n",
    "    # 3) roll up by timestamp\n",
    "    ohe = (\n",
    "        weighted\n",
    "        .groupby(df[\"timestamp\"], as_index=True)\n",
    "        .sum()\n",
    "        .astype(\"uint32\")\n",
    "    )\n",
    "\n",
    "    return ohe\n",
    "\n",
    "\n",
    "def prepend_column(df: pd.DataFrame, name: str, data: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Insert `data` as the first column of `df`.\"\"\"\n",
    "    df.insert(0, name, data)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# LOAD TRANSACTIONS\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "transactions = pd.read_excel(SRC_FILE, parse_dates=[\"timestamp\"])\n",
    "\n",
    "if transactions.isna().any().any():\n",
    "    warnings.warn(\"⚠️ Missing values in transaction file.\")\n",
    "\n",
    "# Standardise text\n",
    "transactions[\"item_class\"]   = transactions[\"item_class\"].str.upper().str.strip()\n",
    "transactions[\"product_name\"] = transactions[\"product_name\"].str.upper().str.strip()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# HOURLY REVENUE  (rounded to 2 decimals)\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "hourly_revenue = (\n",
    "    transactions\n",
    "      .groupby(\"timestamp\")[\"price_aud\"]\n",
    "      .sum()\n",
    "      .round(2)\n",
    "      .rename(\"total_price_aud\")\n",
    ")\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# BUILD OHE MATRICES (count-based)\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "ohe_item_class = build_ohe_matrix(transactions, [\"item_class\"], keep_prefix=False)\n",
    "ohe_product    = build_ohe_matrix(transactions, [\"product_name\"], keep_prefix=False)\n",
    "ohe_combined   = build_ohe_matrix(\n",
    "    transactions, [\"item_class\",\"product_name\"], keep_prefix=True\n",
    ")\n",
    "\n",
    "# prepend revenue\n",
    "for mat in (ohe_item_class, ohe_product, ohe_combined):\n",
    "    prepend_column(mat, \"total_price_aud\", hourly_revenue)\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# SAVE TO EXCEL\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "ohe_item_class.to_excel(FILE_ITEM_CLASS)\n",
    "ohe_product.to_excel   (FILE_PRODUCT_NAME)\n",
    "ohe_combined.to_excel  (FILE_COMBINED)\n",
    "\n",
    "print(\"✓ OHE transaction files written:\")\n",
    "for p in (FILE_ITEM_CLASS, FILE_PRODUCT_NAME, FILE_COMBINED):\n",
    "    print(\"  •\", p.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1729c01-a7c3-4f89-8d9c-cc83982c11a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Total cleaned sessions: 42,728\n",
      "• movie_sessions_clean.xlsx written\n",
      "• movie_sessions_clean_2023.xlsx (19,212 rows)\n",
      "• movie_sessions_clean_2024.xlsx (20,061 rows)\n",
      "• movie_sessions_clean_2025.xlsx (3,455 rows)\n",
      "\n",
      "🎉 Movie‑session cleaning complete – files ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "clean_movie_sessions.py\n",
    "────────────────────────────────────────────────────────────────────────\n",
    "Normalise and filter every workbook that matches the pattern:\n",
    "    Movie_sessions v<X.Y>.xlsx\n",
    "\n",
    "Business rules\n",
    "--------------\n",
    "▪ Exclude sessions whose *Genre* is “GAMING” or “TO BE ADVISED”.\n",
    "▪ Remove records with placeholder runtime `duration_min == 960`.\n",
    "▪ Keep only sessions that start between **09 : 00** and **21 : 59**.\n",
    "▪ Categorise duration as *short / medium / long* and assign a\n",
    "  time‑of‑day *slot* (morning → night_6).\n",
    "\n",
    "Outputs\n",
    "-------\n",
    "movie_sessions_clean.xlsx                 — all years combined\n",
    "movie_sessions_clean_<YEAR>.xlsx          — one file per calendar year\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import Final, Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════╗\n",
    "# CONFIGURATION CONSTANTS\n",
    "# ╚═══════════════════════════════════════════════════════════════╝\n",
    "DATA_DIR: Final[Path]          = Path(\".\")\n",
    "SOURCE_RX: Final[re.Pattern[str]] = re.compile(\n",
    "    r\"Movie_sessions\"                    # Starts with \"Movie_sessions\"\n",
    "    r\"(?:_\"                               # Optional non-capturing group for month/year part\n",
    "        r\"(?:(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s?\\d{4})\"  # Optional: Underscore, Month Name, optional space, Year (e.g., _Feb 2025, _Jan2025)\n",
    "    r\")?\"                                # Make this whole month/year part optional\n",
    "    r\"(?:\\s+v\\d+(?:\\.\\d+)?)?\"            # Optional: space + version (e.g., \" v0.1\", \" v1\")\n",
    "    r\"\\.xlsx$\",                          # Ends with .xlsx\n",
    "    re.I                                 # Case-insensitive\n",
    ")\n",
    "SHEET_NAME: Final[str]         = \"Sheet1\"\n",
    "\n",
    "DROP_COLUMNS: Final[set[str]]  = {\"Film\"}\n",
    "EXCLUDE_GENRES: Final[set[str]] = {\"GAMING\", \"TO BE ADVISED\"}\n",
    "\n",
    "# Cinema trading‑hour slots (minutes after midnight: [start, end) )\n",
    "SLOT_WINDOWS: Final[Dict[str, Tuple[int, int]]] = {\n",
    "    \"morning\":     ( 9*60, 11*60),\n",
    "    \"early_noon\":  (11*60, 13*60),\n",
    "    \"noon\":        (13*60, 15*60),\n",
    "    \"late_noon\":   (15*60, 17*60),\n",
    "    \"evening_1\":   (17*60, 17*60 + 30),\n",
    "    \"evening_2\":   (17*60 + 30, 18*60),\n",
    "    \"evening_3\":   (18*60, 18*60 + 15),\n",
    "    \"evening_4\":   (18*60 + 15, 18*60 + 30),\n",
    "    \"evening_5\":   (18*60 + 30, 18*60 + 45),\n",
    "    \"evening_6\":   (18*60 + 45, 19*60),\n",
    "    \"night_1\":     (19*60, 19*60 + 15),\n",
    "    \"night_2\":     (19*60 + 15, 19*60 + 30),\n",
    "    \"night_3\":     (19*60 + 30, 20*60),\n",
    "    \"night_4\":     (20*60, 20*60 + 30),\n",
    "    \"night_5\":     (20*60 + 30, 21*60),\n",
    "    \"night_6\":     (21*60, 22*60),   # 21:00 – 21:59\n",
    "}\n",
    "\n",
    "# Duration categories (minutes)\n",
    "SHORT_MAX:  Final[int] = 120\n",
    "MEDIUM_MAX: Final[int] = 160\n",
    "\n",
    "CLEAN_COLS: Final[list[str]] = [\n",
    "    \"timestamp\", \"language\", \"genre\", \"rating\",\n",
    "    \"admits\", \"duration_min\", \"duration_category\", \"slot\",\n",
    "]\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════╗\n",
    "# HELPER FUNCTIONS\n",
    "# ╚═══════════════════════════════════════════════════════════════╝\n",
    "def _locate_header_row(xl_path: Path) -> int:\n",
    "    \"\"\"Return the row number containing 'Session Date' (header row).\"\"\"\n",
    "    raw = pd.read_excel(xl_path, sheet_name=SHEET_NAME, header=None, dtype=str)\n",
    "    for idx, row in raw.iterrows():\n",
    "        if any(str(cell).lower().startswith(\"session date\") for cell in row):\n",
    "            return idx\n",
    "    raise ValueError(f\"{xl_path.name}: header row not found\")\n",
    "\n",
    "\n",
    "def _slot_from_timestamp(ts: pd.Timestamp) -> str:\n",
    "    \"\"\"Map a timestamp to its cinema time slot, or 'out_of_range'.\"\"\"\n",
    "    minutes = ts.hour * 60 + ts.minute\n",
    "    for slot, (start, end) in SLOT_WINDOWS.items():\n",
    "        if start <= minutes < end:\n",
    "            return slot\n",
    "    return \"out_of_range\"\n",
    "\n",
    "\n",
    "def _duration_bucket(minutes: int) -> str:\n",
    "    if minutes <= SHORT_MAX:\n",
    "        return \"short\"\n",
    "    if minutes <= MEDIUM_MAX:\n",
    "        return \"medium\"\n",
    "    return \"long\"\n",
    "\n",
    "\n",
    "def _parse_duration(duration_text: str) -> int:\n",
    "    match = re.search(r\"(\\d+)\", str(duration_text))\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════╗\n",
    "# CORE CLEANING ROUTINE\n",
    "# ╚═══════════════════════════════════════════════════════════════╝\n",
    "def clean_one_workbook(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load → clean → return a standardised DataFrame for one workbook.\"\"\"\n",
    "    header_row = _locate_header_row(xl_path)\n",
    "    df = pd.read_excel(xl_path, sheet_name=SHEET_NAME, header=header_row, dtype=str)\n",
    "\n",
    "    # Drop columns we never use (e.g., 'Film')\n",
    "    df = df.drop(columns=DROP_COLUMNS, errors=\"ignore\")\n",
    "\n",
    "    # Parse and validate date / hour columns\n",
    "    df[\"Session Date\"] = pd.to_datetime(df[\"Session Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    df[\"Session Hour\"] = pd.to_numeric(df[\"Session Hour\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Session Date\", \"Session Hour\", \"Genre\", \"Total Admits\", \"Duration\"])\n",
    "    df[\"Session Hour\"] = df[\"Session Hour\"].astype(int)\n",
    "\n",
    "    # Filter out unwanted genres\n",
    "    df = df[~df[\"Genre\"].str.upper().str.strip().isin(EXCLUDE_GENRES)]\n",
    "\n",
    "    # Build timestamp\n",
    "    df[\"timestamp\"] = (\n",
    "        df[\"Session Date\"].dt.normalize() +\n",
    "        pd.to_timedelta(df[\"Session Hour\"], unit=\"h\")\n",
    "    )\n",
    "\n",
    "    # Parse runtime and drop placeholder 960‑min rows\n",
    "    df[\"duration_min\"] = df[\"Duration\"].apply(_parse_duration)\n",
    "    df = df[df[\"duration_min\"] != 960]\n",
    "    df[\"duration_category\"] = df[\"duration_min\"].apply(_duration_bucket)\n",
    "\n",
    "    # Assign cinema slot; keep only trading‑hour rows\n",
    "    df[\"slot\"] = df[\"timestamp\"].apply(_slot_from_timestamp)\n",
    "    df = df[df[\"slot\"] != \"out_of_range\"]\n",
    "\n",
    "    # Standardise column names\n",
    "    df = df.rename(columns={\n",
    "        \"Session Audio Language\": \"language\",\n",
    "        \"Genre\":                  \"genre\",\n",
    "        \"Censor Rating\":          \"rating\",\n",
    "        \"Total Admits\":           \"admits\",\n",
    "    })\n",
    "\n",
    "    return df[CLEAN_COLS].sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════╗\n",
    "# MAIN PIPELINE\n",
    "# ╚═══════════════════════════════════════════════════════════════╝\n",
    "def main() -> None:\n",
    "    workbooks = sorted(p for p in DATA_DIR.iterdir() if SOURCE_RX.fullmatch(p.name))\n",
    "    if not workbooks:\n",
    "        print(\"📭 No source workbooks found – nothing to clean.\")\n",
    "        return\n",
    "\n",
    "    frames = [clean_one_workbook(p) for p in workbooks]\n",
    "    combined = pd.concat(frames, ignore_index=True).sort_values(\"timestamp\")\n",
    "\n",
    "    print(f\"\\n✅ Total cleaned sessions: {len(combined):,}\")\n",
    "\n",
    "    # Combined file\n",
    "    combined.to_excel(\"movie_sessions_clean.xlsx\", index=False)\n",
    "    print(\"• movie_sessions_clean.xlsx written\")\n",
    "\n",
    "    # Yearly splits\n",
    "    for year, grp in combined.groupby(combined[\"timestamp\"].dt.year, sort=True):\n",
    "        fname = f\"movie_sessions_clean_{year}.xlsx\"\n",
    "        grp.to_excel(fname, index=False)\n",
    "        print(f\"• {fname} ({len(grp):,} rows)\")\n",
    "\n",
    "    print(\"\\n🎉 Movie‑session cleaning complete – files ready for analysis.\")\n",
    "\n",
    "\n",
    "# Run automatically in notebook / script\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9595038a-207b-4dee-837e-511c3d1b3822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved hourly-expanded sessions → ohe_movie_sessions_hourly_expanded.xlsx  (11763 rows)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# make_ohe_movie_sessions_hourly_expanded.ipynb\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# Create an **hour‑level** feature table for cinema sessions that\n",
    "# reflects every hour patrons are on‑site, not just the start time.\n",
    "#\n",
    "# Steps\n",
    "# -----\n",
    "# 1.  Expand each session to the hours it overlaps\n",
    "#     • optional `PRE_BUFFER_HRS`  (early‑arrival window)\n",
    "#     • optional `POST_BUFFER_HRS` (linger window)\n",
    "# 2.  One‑hot‑encode categorical fields.\n",
    "# 3.  Aggregate to one row per `timestamp` and compute:\n",
    "#       – total_admits\n",
    "#       – avg_duration_min  (admits‑weighted runtime)\n",
    "#\n",
    "# Output\n",
    "# ------\n",
    "#   ohe_movie_sessions_hourly_expanded.xlsx\n",
    "#\n",
    "# Column order\n",
    "# ------------\n",
    "# timestamp | total_admits | avg_duration_min | <dummy columns …>\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# %%\n",
    "# %%\n",
    "from __future__ import annotations\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from typing import Final, List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CONFIGURATION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# How many hours before a session patrons might arrive\n",
    "PRE_BUFFER_HRS:  Final[int] = 1\n",
    "# How many hours after a session patrons might linger (rarely used)\n",
    "POST_BUFFER_HRS: Final[int] = 0\n",
    "\n",
    "# Path to the cleaned movie-session file and where to write the expanded OHE file\n",
    "PATH_SESS_CLEAN: Final[Path] = Path(\"movie_sessions_clean.xlsx\")\n",
    "OUT_SESS_EXP:    Final[Path] = Path(\"ohe_movie_sessions_hourly_expanded.xlsx\")\n",
    "\n",
    "# Column names\n",
    "TS_COL:   Final[str] = \"timestamp\"\n",
    "ADMITS:   Final[str] = \"total_admits\"\n",
    "AVG_DUR:  Final[str] = \"avg_duration_min\"\n",
    "\n",
    "# Which categorical columns to one-hot encode\n",
    "CAT_COLS: Final[List[str]] = [\n",
    "    \"language\",\n",
    "    \"genre\",\n",
    "    \"rating\",\n",
    "    \"slot\",\n",
    "    \"duration_category\",\n",
    "]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) LOAD & EXPAND EACH SESSION INTO HOURLY “BINS”\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "sessions = pd.read_excel(PATH_SESS_CLEAN, parse_dates=[TS_COL])\n",
    "\n",
    "# Compute when each screening ends\n",
    "sessions[\"end_time\"] = sessions[TS_COL] + pd.to_timedelta(\n",
    "    sessions[\"duration_min\"], unit=\"m\"\n",
    ")\n",
    "\n",
    "expanded_rows: list[pd.Series] = []\n",
    "\n",
    "for _, sess in sessions.iterrows():\n",
    "    real_start = sess[TS_COL]\n",
    "    real_end   = sess[\"end_time\"]\n",
    "\n",
    "    # Define the full window when patrons might be on site\n",
    "    site_start = real_start - timedelta(hours=PRE_BUFFER_HRS)\n",
    "    site_end   = real_end   + timedelta(hours=POST_BUFFER_HRS)\n",
    "\n",
    "    # For every hour mark in that window...\n",
    "    for hr in pd.date_range(site_start, site_end, freq=\"h\", inclusive=\"left\"):\n",
    "        row = sess.copy()\n",
    "        row[TS_COL]    = hr\n",
    "        # Flag whether this hour is within the site-visit window\n",
    "        row[\"in_site\"] = int(site_start <= hr < site_end)\n",
    "        # Flag whether this hour is during the actual show\n",
    "        row[\"in_show\"] = int(real_start <= hr < real_end)\n",
    "        expanded_rows.append(row)\n",
    "\n",
    "# Build expanded DataFrame and drop the temporary end_time\n",
    "expanded = pd.DataFrame(expanded_rows).drop(columns=[\"end_time\"])\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2) ONE-HOT-ENCODE CATEGORIES, ZERO-OUT WHEN NOT IN SHOW, TRACK ADMITS\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "ohe = pd.get_dummies(\n",
    "    expanded,\n",
    "    columns=CAT_COLS,\n",
    "    prefix=CAT_COLS,\n",
    "    prefix_sep=\"_\",\n",
    "    dtype=\"uint8\",\n",
    ")\n",
    "\n",
    "# Find all of our newly created dummy columns\n",
    "cat_dummies = [\n",
    "    c for c in ohe.columns\n",
    "    if any(c.startswith(f\"{cat}_\") for cat in CAT_COLS)\n",
    "]\n",
    "\n",
    "# Zero out category flags for any hour not actually in show\n",
    "ohe[cat_dummies] = ohe[cat_dummies].mul(ohe[\"in_show\"], axis=0)\n",
    "\n",
    "# Compute three key metrics per expanded row:\n",
    "#   1) admits_in_site = how many people are on site this hour\n",
    "#   2) admits_in_show = how many are in the actual screening this hour\n",
    "#   3) dur_x_admits   = duration * admits, but only counted when in_show\n",
    "ohe[\"admits_in_site\"] = ohe[\"admits\"] * ohe[\"in_site\"]\n",
    "ohe[\"admits_in_show\"] = ohe[\"admits\"] * ohe[\"in_show\"]\n",
    "ohe[\"dur_x_admits\"]   = ohe[\"duration_min\"] * ohe[\"admits\"] * ohe[\"in_show\"]\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3) AGGREGATE ALL ROWS TO HOUR-LEVEL\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Build aggregation mapping: sum all dummy flags & our three admits columns\n",
    "agg_map = {c: \"sum\" for c in cat_dummies}\n",
    "agg_map.update({\n",
    "    \"admits_in_site\": \"sum\",\n",
    "    \"admits_in_show\": \"sum\",\n",
    "    \"dur_x_admits\":   \"sum\",\n",
    "})\n",
    "\n",
    "hourly = (\n",
    "    ohe\n",
    "      .groupby(TS_COL, as_index=False)\n",
    "      .agg(agg_map)\n",
    ")\n",
    "\n",
    "# Now compute final columns:\n",
    "#  • total_admits   = admits_in_site\n",
    "#  • avg_duration   = (sum of dur_x_admits) / (sum of admits_in_show)\n",
    "hourly[ADMITS] = hourly[\"admits_in_site\"]\n",
    "hourly[AVG_DUR] = (\n",
    "    hourly[\"dur_x_admits\"]\n",
    "          .div(hourly[\"admits_in_show\"])\n",
    "          .round(1)\n",
    "          .fillna(0)\n",
    ")\n",
    "\n",
    "# Drop our intermediate columns\n",
    "hourly = hourly.drop(\n",
    "    columns=[\n",
    "        \"admits_in_site\",\n",
    "        \"admits_in_show\",\n",
    "        \"dur_x_admits\",\n",
    "        \"in_site\",\n",
    "        \"in_show\",\n",
    "    ],\n",
    "    errors=\"ignore\"\n",
    ")\n",
    "\n",
    "# Reorder so timestamp | total_admits | avg_duration_min | <dummies…>\n",
    "front = [TS_COL, ADMITS, AVG_DUR]\n",
    "hourly = hourly[front + [c for c in hourly.columns if c not in front]]\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4) SAVE TO EXCEL\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "hourly.to_excel(OUT_SESS_EXP, index=False)\n",
    "print(f\"✓ Saved hourly-expanded sessions → {OUT_SESS_EXP.name}  ({len(hourly)} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6be61e0-8e12-4fa2-b81d-bc70d4cf0017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Merged table size: 10,040 rows × 234 columns\n",
      "• Saved train_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# merge_trx_with_expanded_sessions.ipynb\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "# Combine hourly‑level **transaction** features with the hourly‑level,\n",
    "# buffer‑expanded **session** features.\n",
    "#\n",
    "# Join logic\n",
    "# ----------\n",
    "# • Key      : timestamp  (YYYY‑MM‑DD HH:00:00)\n",
    "# • Strategy : LEFT  (retain every hour that contains concession sales)\n",
    "#\n",
    "# Output\n",
    "# ------\n",
    "# merged_trx_sessions_expanded.xlsx\n",
    "#\n",
    "# Final column order\n",
    "# ------------------\n",
    "# timestamp | total_admits | total_price_aud | avg_duration_min | <dummies…>\n",
    "# ─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# %%\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Final\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ╔═════════════════════════════════════════════════════════════════════╗\n",
    "# CONFIGURATION CONSTANTS\n",
    "# ╚═════════════════════════════════════════════════════════════════════╝\n",
    "PATH_TRX_OHE:   Final[Path] = Path(\"ohe_trx_item_class_product.xlsx\")\n",
    "PATH_SESS_OHE:  Final[Path] = Path(\"ohe_movie_sessions_hourly_expanded.xlsx\")\n",
    "PATH_OUT_MERGE: Final[Path] = Path(\"train_dataset.xlsx\")\n",
    "\n",
    "TS:        Final[str] = \"timestamp\"\n",
    "COL_PRICE: Final[str] = \"total_price_aud\"\n",
    "COL_ADMIT: Final[str] = \"total_admits\"\n",
    "COL_DUR:   Final[str] = \"avg_duration_min\"\n",
    "\n",
    "# ╔═════════════════════════════════════════════════════════════════════╗\n",
    "# LOADERS\n",
    "# ╚═════════════════════════════════════════════════════════════════════╝\n",
    "def load_transactions(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load hourly transaction OHE.\n",
    "    Excel saved the timestamp index as the first unnamed column → restore.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(xl_path, index_col=0, parse_dates=[0])\n",
    "    return df.reset_index().rename(columns={\"index\": TS})\n",
    "\n",
    "\n",
    "def load_sessions(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load hourly session OHE (timestamp already present).\"\"\"\n",
    "    return pd.read_excel(xl_path, parse_dates=[TS])\n",
    "\n",
    "\n",
    "def assert_unique_timestamps(df: pd.DataFrame, label: str) -> None:\n",
    "    \"\"\"Fail early if duplicates exist.\"\"\"\n",
    "    dupes = df[df[TS].duplicated()][TS].unique()\n",
    "    if dupes.size:\n",
    "        raise ValueError(f\"{label}: duplicate timestamps {dupes[:5]}…\")\n",
    "\n",
    "\n",
    "def reorder_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Place key numeric columns first; keep original order otherwise.\"\"\"\n",
    "    lead = [TS, COL_ADMIT, COL_PRICE, COL_DUR]\n",
    "    lead = [c for c in lead if c in df.columns]  # tolerate missing cols\n",
    "    return df[lead + [c for c in df.columns if c not in lead]]\n",
    "\n",
    "# ╔═════════════════════════════════════════════════════════════════════╗\n",
    "# MAIN PIPELINE\n",
    "# ╚═════════════════════════════════════════════════════════════════════╝\n",
    "# 1. Load sources\n",
    "trx_df  = load_transactions(PATH_TRX_OHE)\n",
    "sess_df = load_sessions(PATH_SESS_OHE)\n",
    "\n",
    "# 2. Basic sanity checks\n",
    "assert_unique_timestamps(trx_df,  \"Transactions\")\n",
    "assert_unique_timestamps(sess_df, \"Sessions\")\n",
    "\n",
    "# 3. LEFT join (keep every hour with sales)\n",
    "merged = pd.merge(\n",
    "    trx_df,\n",
    "    sess_df,\n",
    "    on=TS,\n",
    "    how=\"left\",\n",
    "    validate=\"one_to_one\",\n",
    ")\n",
    "\n",
    "print(f\"✓ Merged table size: {len(merged):,} rows × {merged.shape[1]} columns\")\n",
    "\n",
    "# 4. Column ordering\n",
    "merged = reorder_columns(merged)\n",
    "\n",
    "# 5. Save\n",
    "merged.to_excel(PATH_OUT_MERGE, index=False)\n",
    "print(f\"• Saved {PATH_OUT_MERGE.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc47e61-d36f-4266-b018-74f3931e8086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

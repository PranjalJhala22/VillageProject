{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb8f58f",
   "metadata": {},
   "source": [
    "# Village Roadshows - Test/Holdout Dataset Cleanup Pipeline\n",
    "\n",
    "This notebook contains a sequential pipeline to process raw data and generate the final `test_dataset.xlsx` file. This final file is used by the `cinema_dashboard.py` application for evaluating recommendation performance.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Inventory Transactions Cleanup:** Reads all raw `Inventory Transaction Data` Excel files from the `input/` directory, cleans them, imputes prices, and saves a consolidated `inventory_transactions_clean.xlsx` to the `output/` directory.\n",
    "2.  **One-Hot Encoding (OHE) of Transactions:** Takes the cleaned inventory data and transforms it into a wide, one-hot encoded format based on item class and product name. This creates several intermediate OHE files in `output/`.\n",
    "3.  **Movie Sessions Cleanup:** Reads the raw `Movie_sessions` file from `input/`, cleans it, and applies business rules (e.g., filtering session times, binning durations).\n",
    "4.  **Hourly Session Expansion:** Expands the cleaned session data so that each session is represented across every hour it is active (including buffer times).\n",
    "5.  **Final Merge:** Combines the OHE transaction data with the expanded session data on a timestamp key to produce the final `test_dataset.xlsx` in the `output/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f02ebe8-97ec-4777-a2bd-0f81d643f2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Data Cleanup Pipeline for 'test_dataset.xlsx' ---\n",
      "\n",
      "\n",
      "--- Stage 1: Cleaning Inventory Transactions ---\n",
      "‚Üí Cleaning inventory file: Inventory Transaction Data Mar 2025 v1.xlsx\n",
      "\n",
      "‚úÖ Cleaned inventory rows total: 7,417\n",
      "  ‚Ä¢ inventory_transactions_clean.xlsx written\n",
      "  ‚Ä¢ inventory_transactions_clean_2025.xlsx  (7,417 rows)\n",
      "\n",
      "--- Stage 2: One-Hot Encoding Transactions ---\n",
      "  ‚Ä¢ ohe_trx_item_class_product.xlsx written\n",
      "\n",
      "--- Stage 3: Cleaning Movie Sessions ---\n",
      "‚Üí Cleaning session file: Movie_sessions_Mar 2025 v1.xlsx\n",
      "\n",
      "‚úÖ Total cleaned sessions: 1,433\n",
      "‚Ä¢ movie_sessions_clean.xlsx written\n",
      "‚Ä¢ movie_sessions_clean_2025.xlsx (1,433 rows)\n",
      "\n",
      "--- Stage 4: Expanding & OHE Movie Sessions ---\n",
      "‚úì Saved hourly-expanded sessions ‚Üí ohe_movie_sessions_hourly_expanded.xlsx  (435 rows)\n",
      "\n",
      "--- Stage 5: Merging to Final Dataset ---\n",
      "‚úì Merged table size: 381 rows √ó 176 columns\n",
      "‚Ä¢ Saved final dataset to: output/test_dataset.xlsx\n",
      "\n",
      "üéâ Pipeline complete. Final file 'test_dataset.xlsx' is in 'output/'.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Village Roadshows: F&B Analytics Data Processing Pipeline.\n",
    "\n",
    "This script serves as the master data engineering workflow, transforming raw\n",
    "transaction and movie session data into a unified, analysis-ready feature matrix.\n",
    "It is designed to be run from a project directory containing 'input/' and\n",
    "'output/' subdirectories.\n",
    "\n",
    "Pipeline Overview:\n",
    "------------------\n",
    "The script executes five sequential stages:\n",
    "\n",
    "1.  **Inventory Transaction Cleanup:**\n",
    "    -   Discovers and loads all raw `Inventory Transaction Data*.xlsx` files.\n",
    "    -   Normalizes inconsistent headers, filters for allowed food & beverage\n",
    "        classes, and standardizes data types.\n",
    "    -   Imputes missing prices using a robust 5-level hierarchical median.\n",
    "    -   Outputs: `inventory_transactions_clean.xlsx` and per-year splits.\n",
    "\n",
    "2.  **Transaction One-Hot Encoding (OHE):**\n",
    "    -   Converts the cleaned, long-format transaction data into a wide-format,\n",
    "        hourly matrix where columns represent individual products.\n",
    "    -   Values are the count of items sold per hour.\n",
    "    -   Prepends a `total_price_aud` column for hourly revenue.\n",
    "    -   Outputs: `ohe_trx_item_class_product.xlsx`\n",
    "\n",
    "3.  **Movie Session Cleanup:**\n",
    "    -   Discovers and loads all raw `Movie_sessions*.xlsx` files.\n",
    "    -   Applies business rules: filters out irrelevant genres, invalid runtimes,\n",
    "        and sessions outside of primary operating hours (09:00 - 21:59).\n",
    "    -   Engineers new features like duration categories and time-of-day slots.\n",
    "    -   Outputs: `movie_sessions_clean.xlsx` and per-year splits.\n",
    "\n",
    "4.  **Hourly Session Expansion & OHE:**\n",
    "    -   Expands each movie session to represent every hour patrons are on-site,\n",
    "        including a pre-session buffer for early arrivals.\n",
    "    -   One-hot encodes categorical session features (genre, rating, etc.).\n",
    "    -   Aggregates data to the hour, calculating total concurrent admissions\n",
    "        and the admit-weighted average movie duration for that hour.\n",
    "    -   Outputs: `ohe_movie_sessions_hourly_expanded.xlsx`\n",
    "\n",
    "5.  **Final Dataset Merge:**\n",
    "    -   Performs a left join, combining the hourly transaction data (Stage 2)\n",
    "        with the expanded hourly session data (Stage 4) on the `timestamp` key.\n",
    "    -   This creates the final feature matrix, keeping all hours with sales.\n",
    "    -   Outputs: `train_dataset.xlsx` (or a similar name).\n",
    "\n",
    "Execution:\n",
    "----------\n",
    "1. Place all raw Excel files into an `input/` directory.\n",
    "2. Run this script from the parent directory.\n",
    "3. All intermediate and final files will be saved to an `output/` directory.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, Final, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# --- 1. CONFIGURATION CONSTANTS ---\n",
    "# =============================================================================\n",
    "# All constants, business rules, and file paths are defined here for easy\n",
    "# maintenance and modification without altering the core script logic.\n",
    "\n",
    "# --- Path Configuration ---\n",
    "BASE_DIR: Final[Path] = Path(\".\")\n",
    "INPUT_DIR: Final[Path] = BASE_DIR / \"input\"\n",
    "OUTPUT_DIR: Final[Path] = BASE_DIR / \"output\"\n",
    "\n",
    "# --- Regex Patterns for File Discovery ---\n",
    "# Matches files like \"Inventory Transaction Data 2023 v0.1.xlsx\" or \"Inventory Transaction Data Feb 2025 v1.xlsx\"\n",
    "WORKBOOK_RX_INV: Final[re.Pattern[str]] = re.compile(\n",
    "    r\"Inventory Transaction Data \"\n",
    "    r\"(?:(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}|\\d{4})\"\n",
    "    r\"(?:\\s+v\\d+(?:\\.\\d+)?)?\"\n",
    "    r\"\\.xlsx$\",\n",
    "    re.I,\n",
    ")\n",
    "# Matches files like \"Movie_sessions v1.2.xlsx\" or \"Movie_sessions_Jan2025.xlsx\"\n",
    "SOURCE_RX_SESS: Final[re.Pattern[str]] = re.compile(\n",
    "    r\"Movie_sessions\"\n",
    "    r\"(?:_(?:(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s?\\d{4}))?\"\n",
    "    r\"(?:\\s+v\\d+(?:\\.\\d+)?)?\"\n",
    "    r\"\\.xlsx$\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "# --- Inventory Cleaning Business Rules ---\n",
    "ALLOWED_CLASSES: Final[set[str]] = {\n",
    "    \"SNACK - CHIPS\", \"FOOD - VJUNIOR\", \"ICE CREAMS - OTHER\", \"ICE CREAMS - CHOC TO\",\n",
    "    \"DRINKS - EXTRA LARGE\", \"DRINKS - LARGE\", \"DRINKS - MEDIUM\", \"DRINKS - SMALL\",\n",
    "    \"DRINKS - NO ICE\", \"DRINKS\", \"POPCORN\",\n",
    "}\n",
    "# Regex to find and remove \"NO ICE\" or \"NO SUGAR\" flags from product names\n",
    "NO_FLAG_RX: Final[re.Pattern[str]] = re.compile(r\"\\bNO\\s+(ICE|SUGAR)\\b\", re.I)\n",
    "\n",
    "# --- Session Cleaning Business Rules ---\n",
    "EXCLUDE_GENRES: Final[set[str]] = {\"GAMING\", \"TO BE ADVISED\"}\n",
    "# Session runtimes of exactly 960 minutes are considered placeholder data\n",
    "PLACEHOLDER_RUNTIME: Final[int] = 960\n",
    "\n",
    "# --- Feature Engineering Constants ---\n",
    "# Time-of-day slots, defined by minutes from midnight: [start, end)\n",
    "SLOT_WINDOWS: Final[Dict[str, Tuple[int, int]]] = {\n",
    "    \"morning\": (9 * 60, 11 * 60), \"early_noon\": (11 * 60, 13 * 60),\n",
    "    \"noon\": (13 * 60, 15 * 60), \"late_noon\": (15 * 60, 17 * 60),\n",
    "    \"evening_1\": (17 * 60, 17 * 60 + 30), \"evening_2\": (17 * 60 + 30, 18 * 60),\n",
    "    \"evening_3\": (18 * 60, 18 * 60 + 15), \"evening_4\": (18 * 60 + 15, 18 * 60 + 30),\n",
    "    \"evening_5\": (18 * 60 + 30, 18 * 60 + 45), \"evening_6\": (18 * 60 + 45, 19 * 60),\n",
    "    \"night_1\": (19 * 60, 19 * 60 + 15), \"night_2\": (19 * 60 + 15, 19 * 60 + 30),\n",
    "    \"night_3\": (19 * 60 + 30, 20 * 60), \"night_4\": (20 * 60, 20 * 60 + 30),\n",
    "    \"night_5\": (20 * 60 + 30, 21 * 60), \"night_6\": (21 * 60, 22 * 60),\n",
    "}\n",
    "# Duration categories for movies (in minutes)\n",
    "SHORT_MAX_SESS: Final[int] = 120\n",
    "MEDIUM_MAX_SESS: Final[int] = 160\n",
    "\n",
    "# Buffer window (in hours) for session expansion\n",
    "PRE_BUFFER_HRS: Final[int] = 1  # How many hours before a session patrons might arrive\n",
    "POST_BUFFER_HRS: Final[int] = 0  # How many hours after a session patrons might linger\n",
    "\n",
    "# Categorical columns to one-hot encode during session expansion\n",
    "CAT_COLS_SESS: Final[List[str]] = [\n",
    "    \"language\", \"genre\", \"rating\", \"slot\", \"duration_category\"\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# --- 2. HELPER FUNCTIONS ---\n",
    "# =============================================================================\n",
    "# Small, reusable utility functions used across multiple pipeline stages.\n",
    "# Underscore prefix indicates they are intended for internal use in this module.\n",
    "\n",
    "def _find_header_row(xl_path: Path, sheet_name: str, start_col_name: str) -> int:\n",
    "    \"\"\"Finds the header row index by searching for a specific column name.\n",
    "\n",
    "    This is necessary because source Excel files may have variable numbers of\n",
    "    title rows before the actual data table.\n",
    "\n",
    "    Args:\n",
    "        xl_path: Path to the Excel workbook.\n",
    "        sheet_name: The name of the sheet to search within.\n",
    "        start_col_name: The text the key header cell starts with (case-insensitive).\n",
    "\n",
    "    Returns:\n",
    "        The 0-based index of the header row.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no row contains the specified header text.\n",
    "    \"\"\"\n",
    "    raw = pd.read_excel(xl_path, sheet_name=sheet_name, header=None, dtype=str)\n",
    "    for idx, row in raw.iterrows():\n",
    "        if any(str(cell).lower().strip().startswith(start_col_name) for cell in row):\n",
    "            return idx\n",
    "    raise ValueError(f\"{xl_path.name}: Header with '{start_col_name}' not found.\")\n",
    "\n",
    "def _remove_no_flag(text: str) -> str:\n",
    "    \"\"\"Strips 'NO ICE'/'NO SUGAR' flags to get the base product name.\"\"\"\n",
    "    return NO_FLAG_RX.sub(\"\", str(text)).replace(\"  \", \" \").strip()\n",
    "\n",
    "def _map_timestamp_to_slot(ts: pd.Timestamp) -> str:\n",
    "    \"\"\"Maps a timestamp to its defined time-of-day slot.\"\"\"\n",
    "    minutes = ts.hour * 60 + ts.minute\n",
    "    for slot, (start, end) in SLOT_WINDOWS.items():\n",
    "        if start <= minutes < end:\n",
    "            return slot\n",
    "    return \"out_of_range\"\n",
    "\n",
    "def _get_duration_category(minutes: int) -> str:\n",
    "    \"\"\"Categorizes a movie's runtime into 'short', 'medium', or 'long'.\"\"\"\n",
    "    if minutes <= SHORT_MAX_SESS: return \"short\"\n",
    "    if minutes <= MEDIUM_MAX_SESS: return \"medium\"\n",
    "    return \"long\"\n",
    "\n",
    "def _parse_runtime_from_text(duration_text: str) -> int:\n",
    "    \"\"\"Extracts the first integer from a text string (e.g., '145 min').\"\"\"\n",
    "    match = re.search(r\"(\\d+)\", str(duration_text))\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def _build_ohe_matrix(df: pd.DataFrame, cat_cols: list[str], *, keep_prefix: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Creates a timestamp-indexed, quantity-weighted OHE matrix.\n",
    "\n",
    "    Args:\n",
    "        df: The input DataFrame, must contain 'timestamp' and 'quantity'.\n",
    "        cat_cols: A list of categorical columns to encode.\n",
    "        keep_prefix: If True, OHE columns will be prefixed (e.g., 'item_class_POPCORN').\n",
    "                     If False, they will not (e.g., 'POPCORN').\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame indexed by 'timestamp' with OHE columns.\n",
    "    \"\"\"\n",
    "    if \"quantity\" not in df.columns:\n",
    "        raise KeyError(\"Input DataFrame must have a 'quantity' column.\")\n",
    "\n",
    "    dummy_kwargs = {} if keep_prefix else dict(prefix=\"\", prefix_sep=\"\")\n",
    "    dummies = pd.get_dummies(df[cat_cols], columns=cat_cols, dtype=\"uint32\", **dummy_kwargs)\n",
    "\n",
    "    # Weight each OHE flag by the number of items sold in that transaction\n",
    "    weighted_dummies = dummies.mul(df[\"quantity\"].values, axis=0)\n",
    "\n",
    "    # Aggregate by hour to get total counts for each category\n",
    "    return weighted_dummies.groupby(df[\"timestamp\"]).sum().astype(\"uint32\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- 3. CORE DATA PROCESSING FUNCTIONS ---\n",
    "# =============================================================================\n",
    "# These functions contain the detailed logic for cleaning a single workbook.\n",
    "\n",
    "def _clean_one_inventory_workbook(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Loads and cleans a single raw inventory transaction workbook.\"\"\"\n",
    "    print(f\"‚Üí Cleaning inventory file: {xl_path.name}\")\n",
    "    header_row = _find_header_row(xl_path, \"Inventory Trans\", \"transaction date\")\n",
    "    df_raw = pd.read_excel(xl_path, sheet_name=\"Inventory Trans\", header=header_row, dtype=str)\n",
    "\n",
    "    # Filter out summary/footer rows\n",
    "    df = df_raw[~df_raw[\"Transaction Date\"].str.contains(\"result\", case=False, na=False)].copy()\n",
    "\n",
    "    # Standardize column names and drop empty 'Unnamed' columns\n",
    "    rename_map = {}\n",
    "    for col in df.columns:\n",
    "        key = str(col).lower().strip()\n",
    "        if \"no of items\" in key or key == \"ea\": rename_map[col] = \"quantity\"\n",
    "        elif \"sell price\" in key or key == \"aud\": rename_map[col] = \"price_aud\"\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    df.drop(columns=[c for c in df.columns if str(c).startswith('Unnamed')], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Parse and build timestamp\n",
    "    df[\"Transaction Date\"] = pd.to_datetime(df[\"Transaction Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    df[\"Transaction Hour\"] = pd.to_numeric(df[\"Transaction Hour\"], errors=\"coerce\")\n",
    "    df.dropna(subset=[\"Transaction Date\", \"Transaction Hour\"], inplace=True)\n",
    "    df[\"timestamp\"] = (\n",
    "        df[\"Transaction Date\"].dt.normalize()\n",
    "        + pd.to_timedelta(df[\"Transaction Hour\"].astype(int), unit=\"h\")\n",
    "    )\n",
    "\n",
    "    # Normalize text fields and filter for allowed item classes\n",
    "    for col in [\"Item Class\", \"VISTA Item\"]:\n",
    "        df[col] = df[col].str.strip()\n",
    "    df[\"Item Class\"] = df[\"Item Class\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df = df[df[\"Item Class\"].str.upper().isin(ALLOWED_CLASSES)].copy()\n",
    "\n",
    "    # Clean numeric columns\n",
    "    df[\"quantity\"] = pd.to_numeric(df[\"quantity\"], errors=\"coerce\").fillna(1).clip(lower=1).astype(int)\n",
    "    df[\"price_aud\"] = pd.to_numeric(df[\"price_aud\"], errors=\"coerce\")\n",
    "\n",
    "    # --- Price Imputation Hierarchy ---\n",
    "    df[\"unit_price\"] = df[\"price_aud\"] / df[\"quantity\"]\n",
    "    unit_price_map = df[df[\"unit_price\"] > 0].drop_duplicates(\"VISTA Item\").set_index(\"VISTA Item\")[\"unit_price\"]\n",
    "\n",
    "    # Fill missing prices using a 5-level cascade\n",
    "    for level in range(5):\n",
    "        missing_mask = df[\"unit_price\"].isna() | (df[\"unit_price\"] <= 0)\n",
    "        if not missing_mask.any(): break\n",
    "\n",
    "        if level == 0: # 1. Exact product name match\n",
    "            df.loc[missing_mask, \"unit_price\"] = df.loc[missing_mask, \"VISTA Item\"].map(unit_price_map)\n",
    "        elif level == 1: # 2. 'NO ICE'/'NO SUGAR' proxy match\n",
    "            df.loc[missing_mask, \"unit_price\"] = df.loc[missing_mask, \"VISTA Item\"].apply(_remove_no_flag).map(unit_price_map)\n",
    "        elif level == 2: # 3. Median price for 'SNACK - CHIPS'\n",
    "            chips_mask = missing_mask & (df[\"Item Class\"] == \"SNACK - CHIPS\")\n",
    "            if chips_mask.any():\n",
    "                chips_median = df.loc[df[\"Item Class\"] == \"SNACK - CHIPS\", \"unit_price\"].median()\n",
    "                df.loc[chips_mask, \"unit_price\"] = chips_median\n",
    "        elif level == 3: # 4. Median price for the item's class\n",
    "            class_medians = df[df[\"unit_price\"] > 0].groupby(\"Item Class\")[\"unit_price\"].median()\n",
    "            df.loc[missing_mask, \"unit_price\"] = df.loc[missing_mask, \"Item Class\"].map(class_medians)\n",
    "        elif level == 4: # 5. Global median price\n",
    "            df.loc[missing_mask, \"unit_price\"] = df[\"unit_price\"].median(skipna=True)\n",
    "    \n",
    "    # Recalculate total price after imputation\n",
    "    df[\"price_aud\"] = (df[\"unit_price\"] * df[\"quantity\"]).round(2)\n",
    "\n",
    "    # Final selection and renaming\n",
    "    return df.rename(columns={\"Item Class\": \"item_class\", \"VISTA Item\": \"product_name\"})[\n",
    "        [\"timestamp\", \"item_class\", \"product_name\", \"quantity\", \"price_aud\"]\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "def _clean_one_session_workbook(xl_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Loads, cleans, and engineers features for a single session workbook.\"\"\"\n",
    "    print(f\"‚Üí Cleaning session file: {xl_path.name}\")\n",
    "    header_row = _find_header_row(xl_path, \"Sheet1\", \"session date\")\n",
    "    df = pd.read_excel(xl_path, sheet_name=\"Sheet1\", header=header_row, dtype=str)\n",
    "\n",
    "    # Initial cleanup and type conversion\n",
    "    df = df.drop(columns={\"Film\"}, errors=\"ignore\")\n",
    "    df[\"Session Date\"] = pd.to_datetime(df[\"Session Date\"], dayfirst=True, errors=\"coerce\")\n",
    "    df[\"Session Hour\"] = pd.to_numeric(df[\"Session Hour\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Session Date\", \"Session Hour\", \"Genre\", \"Total Admits\", \"Duration\"])\n",
    "\n",
    "    # Apply business rule filters\n",
    "    df = df[~df[\"Genre\"].str.upper().str.strip().isin(EXCLUDE_GENRES)]\n",
    "    df[\"duration_min\"] = df[\"Duration\"].apply(_parse_runtime_from_text)\n",
    "    df = df[df[\"duration_min\"] != PLACEHOLDER_RUNTIME]\n",
    "    \n",
    "    # Feature Engineering\n",
    "    df[\"timestamp\"] = (\n",
    "        df[\"Session Date\"].dt.normalize()\n",
    "        + pd.to_timedelta(df[\"Session Hour\"].astype(int), unit=\"h\")\n",
    "    )\n",
    "    df[\"duration_category\"] = df[\"duration_min\"].apply(_get_duration_category)\n",
    "    df[\"slot\"] = df[\"timestamp\"].apply(_map_timestamp_to_slot)\n",
    "    \n",
    "    # Keep only sessions within defined trading hours\n",
    "    df = df[df[\"slot\"] != \"out_of_range\"].copy()\n",
    "\n",
    "    # Final renaming and column selection\n",
    "    df.rename(columns={\n",
    "        \"Session Audio Language\": \"language\", \"Genre\": \"genre\",\n",
    "        \"Censor Rating\": \"rating\", \"Total Admits\": \"admits\"\n",
    "    }, inplace=True)\n",
    "    df[\"admits\"] = pd.to_numeric(df[\"admits\"], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    return df[\n",
    "        [\"timestamp\", \"language\", \"genre\", \"rating\", \"admits\",\n",
    "         \"duration_min\", \"duration_category\", \"slot\"]\n",
    "    ].sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "# =============================================================================\n",
    "# --- 4. PIPELINE STAGE FUNCTIONS ---\n",
    "# =============================================================================\n",
    "# Each function here corresponds to a major stage in the data pipeline.\n",
    "\n",
    "def run_inventory_cleaning_stage(input_dir: Path, output_dir: Path) -> Path | None:\n",
    "    \"\"\"Stage 1: Discovers, cleans, and consolidates all inventory files.\"\"\"\n",
    "    print(\"\\n--- Stage 1: Cleaning Inventory Transactions ---\")\n",
    "    source_files = sorted(p for p in input_dir.iterdir() if WORKBOOK_RX_INV.fullmatch(p.name))\n",
    "    if not source_files:\n",
    "        print(f\"üì≠ No source inventory workbooks found in '{input_dir}'. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    all_frames = [_clean_one_inventory_workbook(p) for p in source_files]\n",
    "    master_df = pd.concat(all_frames, ignore_index=True).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    print(f\"\\n‚úÖ Cleaned inventory rows total: {len(master_df):,}\")\n",
    "\n",
    "    # Save the consolidated master file\n",
    "    master_output_path = output_dir / \"inventory_transactions_clean.xlsx\"\n",
    "    master_df.to_excel(master_output_path, index=False)\n",
    "    print(f\"  ‚Ä¢ {master_output_path.name} written\")\n",
    "\n",
    "    # Save per-year splits for easier ad-hoc analysis\n",
    "    for year, group in master_df.groupby(master_df[\"timestamp\"].dt.year):\n",
    "        fname = f\"inventory_transactions_clean_{year}.xlsx\"\n",
    "        group.to_excel(output_dir / fname, index=False)\n",
    "        print(f\"  ‚Ä¢ {fname}  ({len(group):,} rows)\")\n",
    "        \n",
    "    return master_output_path\n",
    "\n",
    "def run_transaction_ohe_stage(input_path: Path, output_dir: Path) -> None:\n",
    "    \"\"\"Stage 2: Creates a one-hot encoded matrix from cleaned transactions.\"\"\"\n",
    "    print(\"\\n--- Stage 2: One-Hot Encoding Transactions ---\")\n",
    "    if not input_path or not input_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Source file not found at '{input_path}'. Skipping OHE stage.\")\n",
    "        return\n",
    "\n",
    "    transactions = pd.read_excel(input_path, parse_dates=[\"timestamp\"])\n",
    "    transactions[\"item_class\"] = transactions[\"item_class\"].str.upper().str.strip()\n",
    "    transactions[\"product_name\"] = transactions[\"product_name\"].str.upper().str.strip()\n",
    "    \n",
    "    hourly_revenue = transactions.groupby(\"timestamp\")[\"price_aud\"].sum().round(2).rename(\"total_price_aud\")\n",
    "    ohe_combined = _build_ohe_matrix(transactions, [\"item_class\", \"product_name\"], keep_prefix=True)\n",
    "    ohe_combined.insert(0, \"total_price_aud\", hourly_revenue)\n",
    "\n",
    "    output_path = output_dir / \"ohe_trx_item_class_product.xlsx\"\n",
    "    ohe_combined.to_excel(output_path)\n",
    "    print(f\"  ‚Ä¢ {output_path.name} written\")\n",
    "\n",
    "def run_session_cleaning_stage(input_dir: Path, output_dir: Path) -> Path | None:\n",
    "    \"\"\"Stage 3: Discovers, cleans, and consolidates all movie session files.\"\"\"\n",
    "    print(\"\\n--- Stage 3: Cleaning Movie Sessions ---\")\n",
    "    source_files = sorted(p for p in input_dir.iterdir() if SOURCE_RX_SESS.fullmatch(p.name))\n",
    "    if not source_files:\n",
    "        print(f\"üì≠ No source movie session workbooks found in '{input_dir}'. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    all_frames = [_clean_one_session_workbook(p) for p in source_files]\n",
    "    master_df = pd.concat(all_frames, ignore_index=True).sort_values(\"timestamp\")\n",
    "    print(f\"\\n‚úÖ Total cleaned sessions: {len(master_df):,}\")\n",
    "\n",
    "    master_output_path = output_dir / \"movie_sessions_clean.xlsx\"\n",
    "    master_df.to_excel(master_output_path, index=False)\n",
    "    print(f\"‚Ä¢ {master_output_path.name} written\")\n",
    "\n",
    "    for year, group in master_df.groupby(master_df[\"timestamp\"].dt.year):\n",
    "        fname = f\"movie_sessions_clean_{year}.xlsx\"\n",
    "        group.to_excel(output_dir / fname, index=False)\n",
    "        print(f\"‚Ä¢ {fname} ({len(group):,} rows)\")\n",
    "        \n",
    "    return master_output_path\n",
    "\n",
    "def run_session_expansion_stage(input_path: Path, output_dir: Path) -> Path | None:\n",
    "    \"\"\"Stage 4: Expands session data to an hourly level and OHE encodes it.\"\"\"\n",
    "    print(\"\\n--- Stage 4: Expanding & OHE Movie Sessions ---\")\n",
    "    if not input_path or not input_path.exists():\n",
    "        print(f\"‚ö†Ô∏è Source file not found at '{input_path}'. Skipping expansion stage.\")\n",
    "        return None\n",
    "\n",
    "    sessions = pd.read_excel(input_path, parse_dates=[\"timestamp\"])\n",
    "    sessions[\"end_time\"] = sessions[\"timestamp\"] + pd.to_timedelta(sessions[\"duration_min\"], unit=\"m\")\n",
    "\n",
    "    # Explode each session row into multiple rows, one for each hour it's active\n",
    "    expanded_rows = []\n",
    "    for _, sess in sessions.iterrows():\n",
    "        site_start = sess[\"timestamp\"] - timedelta(hours=PRE_BUFFER_HRS)\n",
    "        site_end = sess[\"end_time\"] + timedelta(hours=POST_BUFFER_HRS)\n",
    "        for hr in pd.date_range(site_start, site_end, freq=\"h\", inclusive=\"left\"):\n",
    "            row = sess.copy()\n",
    "            row[\"timestamp\"] = hr\n",
    "            row[\"in_show\"] = int(sess[\"timestamp\"] <= hr < sess[\"end_time\"]) # Flag if movie is running\n",
    "            expanded_rows.append(row)\n",
    "    expanded = pd.DataFrame(expanded_rows).drop(columns=[\"end_time\"])\n",
    "    \n",
    "    # OHE and prepare for aggregation\n",
    "    ohe = pd.get_dummies(expanded, columns=CAT_COLS_SESS, prefix=CAT_COLS_SESS, prefix_sep=\"_\", dtype=\"uint8\")\n",
    "    cat_dummies = [c for c in ohe.columns if any(c.startswith(f\"{cat}_\") for cat in CAT_COLS_SESS)]\n",
    "    ohe[cat_dummies] = ohe[cat_dummies].mul(ohe[\"in_show\"], axis=0)\n",
    "    \n",
    "    # Calculate admits-weighted metrics for aggregation\n",
    "    ohe[\"admits_in_show\"] = ohe[\"admits\"] * ohe[\"in_show\"]\n",
    "    ohe[\"dur_x_admits\"] = ohe[\"duration_min\"] * ohe[\"admits_in_show\"]\n",
    "\n",
    "    # Aggregate all data to the hourly level\n",
    "    agg_map = {col: \"sum\" for col in cat_dummies}\n",
    "    agg_map.update({\"admits\": \"sum\", \"admits_in_show\": \"sum\", \"dur_x_admits\": \"sum\"})\n",
    "    hourly = ohe.groupby(\"timestamp\", as_index=False).agg(agg_map)\n",
    "    \n",
    "    # Calculate final hourly metrics\n",
    "    hourly.rename(columns={\"admits\": \"total_admits\"}, inplace=True)\n",
    "    hourly[\"avg_duration_min\"] = hourly[\"dur_x_admits\"].div(hourly[\"admits_in_show\"]).round(1).fillna(0)\n",
    "    \n",
    "    # Final cleanup and column ordering\n",
    "    final_cols = [\"timestamp\", \"total_admits\", \"avg_duration_min\"] + cat_dummies\n",
    "    hourly = hourly[final_cols]\n",
    "    \n",
    "    output_path = output_dir / \"ohe_movie_sessions_hourly_expanded.xlsx\"\n",
    "    hourly.to_excel(output_path, index=False)\n",
    "    print(f\"‚úì Saved hourly-expanded sessions ‚Üí {output_path.name}  ({len(hourly)} rows)\")\n",
    "    return output_path\n",
    "\n",
    "def run_final_merge_stage(trx_ohe_path: Path, sess_ohe_path: Path, output_dir: Path, output_filename: str) -> None:\n",
    "    \"\"\"Stage 5: Merges OHE transaction and session data into the final dataset.\"\"\"\n",
    "    print(\"\\n--- Stage 5: Merging to Final Dataset ---\")\n",
    "    if not (trx_ohe_path and trx_ohe_path.exists()):\n",
    "        print(f\"‚ö†Ô∏è Transaction OHE file not found at '{trx_ohe_path}'. Cannot perform merge.\")\n",
    "        return\n",
    "    if not (sess_ohe_path and sess_ohe_path.exists()):\n",
    "        print(f\"‚ö†Ô∏è Session OHE file not found at '{sess_ohe_path}'. Cannot perform merge.\")\n",
    "        return\n",
    "\n",
    "    trx_df = pd.read_excel(trx_ohe_path, index_col=0, parse_dates=[0]).reset_index().rename(columns={\"index\": \"timestamp\"})\n",
    "    sess_df = pd.read_excel(sess_ohe_path, parse_dates=[\"timestamp\"])\n",
    "    \n",
    "    # Validate data integrity before merging\n",
    "    if trx_df[\"timestamp\"].duplicated().any(): raise ValueError(\"Duplicate timestamps found in transaction data.\")\n",
    "    if sess_df[\"timestamp\"].duplicated().any(): raise ValueError(\"Duplicate timestamps found in session data.\")\n",
    "\n",
    "    # Left join to keep every hour with a sale, and add corresponding session data\n",
    "    merged = pd.merge(trx_df, sess_df, on=\"timestamp\", how=\"left\", validate=\"one_to_one\")\n",
    "    print(f\"‚úì Merged table size: {len(merged):,} rows √ó {merged.shape[1]} columns\")\n",
    "    \n",
    "    # Reorder key columns to the front for better readability\n",
    "    lead_cols = [\"timestamp\", \"total_admits\", \"total_price_aud\", \"avg_duration_min\"]\n",
    "    lead_cols_exist = [c for c in lead_cols if c in merged.columns]\n",
    "    merged = merged[lead_cols_exist + [c for c in merged.columns if c not in lead_cols_exist]]\n",
    "    \n",
    "    final_output_path = output_dir / output_filename\n",
    "    merged.to_excel(final_output_path, index=False)\n",
    "    print(f\"‚Ä¢ Saved final dataset to: {final_output_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- 5. MAIN PIPELINE WORKFLOW ---\n",
    "# =============================================================================\n",
    "\n",
    "def run_pipeline(output_filename: str = \"train_dataset.xlsx\"):\n",
    "    \"\"\"\n",
    "    Executes the full data processing pipeline from raw files to the final dataset.\n",
    "\n",
    "    Args:\n",
    "        output_filename: The name for the final merged dataset file.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Data Cleanup Pipeline for '{output_filename}' ---\\n\")\n",
    "    \n",
    "    # Ensure input/output directories exist\n",
    "    INPUT_DIR.mkdir(exist_ok=True)\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Stage 1: Clean Inventory\n",
    "    cleaned_inventory_path = run_inventory_cleaning_stage(INPUT_DIR, OUTPUT_DIR)\n",
    "    \n",
    "    # Stage 2: OHE Transactions\n",
    "    run_transaction_ohe_stage(cleaned_inventory_path, OUTPUT_DIR)\n",
    "    \n",
    "    # Stage 3: Clean Sessions\n",
    "    cleaned_sessions_path = run_session_cleaning_stage(INPUT_DIR, OUTPUT_DIR)\n",
    "    \n",
    "    # Stage 4: Expand and OHE Sessions\n",
    "    expanded_sessions_path = run_session_expansion_stage(cleaned_sessions_path, OUTPUT_DIR)\n",
    "    \n",
    "    # Stage 5: Final Merge\n",
    "    trx_ohe_final_path = OUTPUT_DIR / \"ohe_trx_item_class_product.xlsx\"\n",
    "    run_final_merge_stage(trx_ohe_final_path, expanded_sessions_path, OUTPUT_DIR, output_filename)\n",
    "    \n",
    "    print(f\"\\nüéâ Pipeline complete. Final file '{output_filename}' is in '{OUTPUT_DIR}/'.\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- SCRIPT ENTRY POINT ---\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Determines the output filename based on the name of the directory it's in.\n",
    "    # This allows the same script to be used for 'training' and 'testing' datasets.\n",
    "    current_folder_name = BASE_DIR.resolve().name.lower()\n",
    "    \n",
    "    if \"train\" in current_folder_name:\n",
    "        final_file = \"train_dataset.xlsx\"\n",
    "    elif \"test\" in current_folder_name:\n",
    "        final_file = \"test_dataset.xlsx\"\n",
    "    else:\n",
    "        final_file = \"processed_dataset.xlsx\"\n",
    "        warnings.warn(f\"Could not determine context from folder name. Defaulting output to '{final_file}'.\")\n",
    "    \n",
    "    run_pipeline(output_filename=final_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
